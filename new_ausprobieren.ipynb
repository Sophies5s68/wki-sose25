{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f385f6a-f913-40a9-a0d0-0b50fbe42dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ich probier jetzt jeden Schritt nochmal von vorne und schau mir den Schritt nochmal genauer an'\n",
    "\n",
    "from wettbewerb import load_references, get_6montages\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "\n",
    "\n",
    "ids, channels_list, data_list, fs_list, ref_list, label_list = load_references(folder=\"../shared_data/training\", idx=0)\n",
    "for i in range(100):\n",
    "    #montage_names, montage_data, missing = get_6montages(channels_list[i], data_list[i])\n",
    "    #print(f\"{ids[i]}:{montage_names}:{montage_data.shape}\\n {fs_list[i]}\")\n",
    "    #if missing:\n",
    "        #print(\"Warning: Montage missing, data may be incomplete.\")\n",
    "    #print (f\"{ids[i]}\",label_list[i])\n",
    "    processed_signal, montage_missing = preprocess_signal_with_montages(channels_list[i], data_list[i], 256, fs_list[i])\n",
    "    print(f\"{ids[i]}:\",processed_signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdc1a4-54fe-4817-b565-0d19f94762ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "labels = dataset.get_labels()\n",
    "\n",
    "# Count seizure / non-seizure\n",
    "seizure_count = sum(1 for l in labels if l[0])\n",
    "non_seizure_count = len(labels) - seizure_count\n",
    "\n",
    "print(f\"Total: {len(labels)}\")\n",
    "print(f\"Seizures: {seizure_count}\")\n",
    "print(f\"Non-Seizures: {non_seizure_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d046948-7f78-4882-81c4-19a37e08f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "import os\n",
    "import torch \n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = \"preprocessed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for i in range(total):\n",
    "    ids, channels, data, fs, ref, label = dataset[i]\n",
    "\n",
    "    montage_names, processed_signal, montage_missing, resampled_fs = preprocess_signal_with_montages(\n",
    "        channels, data, target_fs=256, original_fs=fs, ids=ids\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        skipped_count += 1\n",
    "        print(f\"[{i+1}/{total}] Skipping {ids} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{ids}.pt\")\n",
    "    torch.save((processed_signal, label, ids, montage_names, resampled_fs), save_path)\n",
    "    processed_count += 1\n",
    "    print(f\"[{i+1}/{total}] Processed: {processed_count} | Skipped: {skipped_count}\", end='\\r')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0c4a3-d55d-4088-a3b1-ed0ef2c21959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data, feature_extraction_window  # your modules\n",
    "import os, torch\n",
    "import numpy as np\n",
    "\n",
    "window_size = 4  # seconds\n",
    "step_size = 2    # seconds\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = f\"montage_datasets/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # 1. Preprocess\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=256, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # 2. Windowing + labeling\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal, resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    # 3. Feature extraction per window\n",
    "    for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps)):\n",
    "        features = feature_extraction_window(window, new_fs)\n",
    "        save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "        torch.save((features, lbl, eeg_id, ts), save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] Processed {eeg_id} with {len(windows)} windows.\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ae54-6c8b-4551-82d9-6e7ec1c6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen der Features in zeitliche und spektrale\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "def split_features(feature_tensor):\n",
    "    \"\"\"\n",
    "    Trennt Features in spektral (0-9) und zeitlich (10-14)\n",
    "    \"\"\"\n",
    "    spectral = feature_tensor[..., :10]         # Indizes 0-9\n",
    "    temporal = feature_tensor[..., 10:15]       # Indizes 10-14\n",
    "    return spectral, temporal\n",
    "\n",
    "def process_feature_files(load_dir, save_dir_spectral, save_dir_temporal):\n",
    "    os.makedirs(save_dir_spectral, exist_ok=True)\n",
    "    os.makedirs(save_dir_temporal, exist_ok=True)\n",
    "\n",
    "    feature_files = glob(os.path.join(load_dir, \"*.pt\"))\n",
    "\n",
    "    for file in feature_files:\n",
    "        data = torch.load(file)\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            features, label, eeg_id, ts = data\n",
    "        elif isinstance(data, dict):\n",
    "            features = data['features']\n",
    "            label = data['label']\n",
    "            eeg_id = data['eeg_id']\n",
    "            ts = data['timestamp']\n",
    "        else:\n",
    "            print(f\"Unbekanntes Format: {file}\")\n",
    "            continue\n",
    "\n",
    "        # in numpy falls tensor\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = features.numpy()\n",
    "\n",
    "        # flach oder Matrix?\n",
    "        if features.ndim == 1:\n",
    "            n_channels = 6  # oder dein tatsächlicher Wert\n",
    "            features = features.reshape(n_channels, -1)\n",
    "\n",
    "        spec_feat, time_feat = split_features(features)\n",
    "\n",
    "        # Optional: flatten\n",
    "        spec_feat_flat = spec_feat.flatten()\n",
    "        time_feat_flat = time_feat.flatten()\n",
    "\n",
    "        base_name = os.path.basename(file)\n",
    "\n",
    "        # Speichern\n",
    "        torch.save((spec_feat_flat, label, eeg_id, ts), os.path.join(save_dir_spectral, base_name))\n",
    "        torch.save((time_feat_flat, label, eeg_id, ts), os.path.join(save_dir_temporal, base_name))\n",
    "\n",
    "    print(f\"Fertig. {len(feature_files)} Dateien verarbeitet.\")\n",
    "\n",
    "# Beispiel:\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "    \n",
    "    load_dir = \"montage_datasets/\"+ config\n",
    "    save_dir_spectral = \"data_features_sep/spectral/\" + config\n",
    "    save_dir_temporal = \"data_features_sep/temporal/\" + config\n",
    "\n",
    "    process_feature_files(load_dir, save_dir_spectral, save_dir_temporal)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da117929-d02f-4838-b337-bcb0150ccbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config win3_step3 gespeichert.\n",
      "config win2_step2 gespeichert.\n",
      "config win1_step1 gespeichert.\n",
      "config win1_step0.5 gespeichert.\n",
      "config win3_step1 gespeichert.\n",
      "config win2_step1 gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Code zum Zusammenführen von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/add_dataset/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"add_dataset/\" + config\n",
    "    target_dir = \"add_dataset/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc7a116-ca5b-43f6-b518-62d0eebd9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# VERSION HAT FUNKTIONIERT\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sps\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model_copy import CNN_EEG\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from features_prediction import window_prediction, feature_extraction_window\n",
    "#from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "from scipy.signal import iirnotch, butter, sosfiltfilt, resample_poly, tf2sos\n",
    "\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht verändert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der übergebenen Kanäle\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kanäle\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enthält Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bezüglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bezüglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4\n",
    "    step_size = 1\n",
    "    target_fs = 256\n",
    "    original_fs = fs\n",
    " \n",
    "    \n",
    "    montage_names, montage_data, montage_missing,target_fs = preprocess_signal_with_montages(channels, data,target_fs,original_fs) \n",
    "    \n",
    "    windows, timestamps = window_prediction(montage_data, target_fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction_window(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        x = torch.tensor(features, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        probs = predictions_ensemble(data_for_class ,model_name, device)\n",
    "        predictions_per_window = [int(p > 0.5) for p in probs]\n",
    "\n",
    "    seizure_present = False\n",
    "    seizure_present, onset_candidate = detect_onset(predictions_per_window, timestamps, min_consecutive=2)\n",
    "    if seizure_present:\n",
    "        onset = onset_candidate\n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unverändert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(data_for_class: List[torch.Tensor], model_name: str, device: torch.device) -> List[float]:\n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "    batch_tensor = torch.stack(data_for_class).to(device)\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = CNN_EEG(6, 1).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "            outputs = torch.sigmoid(model(batch_tensor).squeeze())\n",
    "            probs.append(outputs.cpu().numpy())  # shape: (num_windows,)\n",
    "\n",
    "    ensemble_probs = np.mean(probs, axis=0)  # Mittelwert pro Fenster\n",
    "    return ensemble_probs.tolist()  # Gib Liste von Wahrscheinlichkeiten zurück\n",
    "\n",
    "\n",
    "def detect_onset(predictions, timestamps, min_consecutive=2):\n",
    "    predictions = torch.tensor(predictions)\n",
    "    for i in range(len(predictions) - min_consecutive + 1):\n",
    "        if torch.all(predictions[i:i+min_consecutive] == 1):\n",
    "            return True, timestamps[i]\n",
    "    return False, None\n",
    "\n",
    "\n",
    "\n",
    "def notch_filter(signal, fs, freq=50.0, Q=30.0):\n",
    "    w0 = freq / (fs / 2)\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    sos = tf2sos(b, a)  # Transferfunktion → SOS\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "\n",
    "def bandpass_filter(signal, fs, lowcut=1.0, highcut=120.0, order=4):\n",
    "    sos = sps.butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs=256):\n",
    "    if original_fs == target_fs:\n",
    "        return signal\n",
    "    gcd = np.gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // gcd)\n",
    "    down = int(original_fs // gcd)\n",
    "    return resample_poly(signal, up, down, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc420f0-3c75-4a50-977a-5a233789ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t Dateien wurden geladen.\n",
      "[(1, 26.08, 50.1025), (1, 2.9212, 32.3607), (0, 0.0, 0.0), (1, 34.8275, 63.0425), (1, 9.1971, 23.5505), (1, 5.285, 26.4575), (1, 19.7015, 28.5202), (0, 0.0, 0.0), (0, 0.0, 0.0), (0, 0.0, 0.0)]\n",
      "{'seizure_present': True, 'seizure_confidence': 0.5, 'onset': 42.0, 'onset_confidence': 0.99, 'offset': 999999, 'offset_confidence': 0}\n"
     ]
    }
   ],
   "source": [
    "from wettbewerb import load_references\n",
    "train_folder = \"../shared_data/training_mini\" \n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder,90)\n",
    "print(eeg_labels)\n",
    "idx = ids[3]\n",
    "channel = channels[3]\n",
    "data_s = data[3]\n",
    "fs = sampling_frequencies[3]\n",
    "ref = reference_systems[3]\n",
    "model_abgabe = \"model_abgabe/\"\n",
    "prediction = predict_labels(channel, data_s, fs, ref, model_abgabe)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1298fb-03e9-455a-b338-45e7370e768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sps\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model_copy import CNN_EEG\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from faster_features import window_prediction, feature_extraction_window\n",
    "#from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "from scipy.signal import iirnotch, butter, sosfiltfilt, resample_poly, tf2sos\n",
    "\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht verändert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der übergebenen Kanäle\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kanäle\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enthält Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bezüglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bezüglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4\n",
    "    step_size = 1\n",
    "    target_fs = 256\n",
    "    original_fs = fs\n",
    " \n",
    "    \n",
    "    montage_names, montage_data, montage_missing,target_fs = preprocess_signal_with_montages(channels, data,target_fs,original_fs) \n",
    "    \n",
    "    windows, timestamps = window_prediction(montage_data, target_fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction_window(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        x = torch.tensor(features, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        probs = predictions_ensemble(data_for_class ,model_name, device)\n",
    "        predictions_per_window = [int(p > 0.5) for p in probs]\n",
    "\n",
    "    seizure_present = False\n",
    "    seizure_present, onset_candidate = detect_onset(predictions_per_window, timestamps, min_consecutive=2)\n",
    "    if seizure_present:\n",
    "        onset = onset_candidate\n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unverändert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(data_for_class: List[torch.Tensor], model_name: str, device: torch.device) -> List[float]:\n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "    batch_tensor = torch.stack(data_for_class).to(device)\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = CNN_EEG(6, 1).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "            outputs = torch.sigmoid(model(batch_tensor).squeeze())\n",
    "            probs.append(outputs.cpu().numpy())  # shape: (num_windows,)\n",
    "\n",
    "    ensemble_probs = np.mean(probs, axis=0)  # Mittelwert pro Fenster\n",
    "    return ensemble_probs.tolist()  # Gib Liste von Wahrscheinlichkeiten zurück\n",
    "\n",
    "\n",
    "def detect_onset(predictions, timestamps, min_consecutive=2):\n",
    "    predictions = torch.tensor(predictions)\n",
    "    for i in range(len(predictions) - min_consecutive + 1):\n",
    "        if torch.all(predictions[i:i+min_consecutive] == 1):\n",
    "            return True, timestamps[i]\n",
    "    return False, None\n",
    "\n",
    "\n",
    "\n",
    "def notch_filter(signal, fs, freq=50.0, Q=30.0):\n",
    "    w0 = freq / (fs / 2)\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    sos = tf2sos(b, a)  # Transferfunktion → SOS\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "\n",
    "def bandpass_filter(signal, fs, lowcut=1.0, highcut=120.0, order=4):\n",
    "    sos = sps.butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs=256):\n",
    "    if original_fs == target_fs:\n",
    "        return signal\n",
    "    gcd = np.gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // gcd)\n",
    "    down = int(original_fs // gcd)\n",
    "    return resample_poly(signal, up, down, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c6697f-0e98-4f87-9e8a-2a98005a0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t Dateien wurden geladen.\n",
      "[(1, 26.08, 50.1025), (1, 2.9212, 32.3607), (0, 0.0, 0.0), (1, 34.8275, 63.0425), (1, 9.1971, 23.5505), (1, 5.285, 26.4575), (1, 19.7015, 28.5202), (0, 0.0, 0.0), (0, 0.0, 0.0), (0, 0.0, 0.0)]\n",
      "{'seizure_present': True, 'seizure_confidence': 0.5, 'onset': 41.0, 'onset_confidence': 0.99, 'offset': 999999, 'offset_confidence': 0}\n"
     ]
    }
   ],
   "source": [
    "from wettbewerb import load_references\n",
    "train_folder = \"../shared_data/training_mini\" \n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder,90)\n",
    "print(eeg_labels)\n",
    "idx = ids[3]\n",
    "channel = channels[3]\n",
    "data_s = data[3]\n",
    "fs = sampling_frequencies[3]\n",
    "ref = reference_systems[3]\n",
    "model_abgabe = \"model_abgabe/\"\n",
    "prediction = predict_labels(channel, data_s, fs, ref, model_abgabe)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495f84f-0191-4e11-b654-09b96388d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code zum Zusammenführen von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"montage_datasets/\" + config\n",
    "    target_dir = \"montage_datasets/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310810f-d1ec-4b7a-94f6-1ee0ba545462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "# === Einstellungen ===\n",
    "input_root = \"montage_datasets/combined/\"\n",
    "output_root_spectral = \"montage_datasets/spectral_only/\"\n",
    "output_root_temporal = \"montage_datasets/temporal_only/\"\n",
    "\n",
    "# Erstelle Zielverzeichnisse, wenn nicht vorhanden\n",
    "os.makedirs(output_root_spectral, exist_ok=True)\n",
    "os.makedirs(output_root_temporal, exist_ok=True)\n",
    "\n",
    "# Alle Konfigurations-Unterordner finden\n",
    "configs = [f for f in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, f))]\n",
    "\n",
    "for config in configs:\n",
    "    input_dir = os.path.join(input_root, config)\n",
    "    output_dir_spec = os.path.join(output_root_spectral, config)\n",
    "    output_dir_temp = os.path.join(output_root_temporal, config)\n",
    "\n",
    "    os.makedirs(output_dir_spec, exist_ok=True)\n",
    "    os.makedirs(output_dir_temp, exist_ok=True)\n",
    "\n",
    "    pt_files = sorted(glob(os.path.join(input_dir, \"*.pt\")))\n",
    "\n",
    "    for file_path in pt_files:\n",
    "        try:\n",
    "            samples = torch.load(file_path)  # List of (channels x 15) matrices\n",
    "\n",
    "            spectral_list = []\n",
    "            temporal_list = []\n",
    "\n",
    "            for sample in samples:\n",
    "                feature_matrix = sample[0]  # (channels x 15)\n",
    "                if isinstance(feature_matrix, np.ndarray):\n",
    "                    feature_matrix = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "                print(f\"feature_matrix shape: {feature_matrix.shape}, dtype: {feature_matrix.dtype}\")\n",
    "                spectral = torch.cat([feature_matrix[:, :10], feature_matrix[:, 13:14]], dim=1)\n",
    "                temporal = torch.cat([feature_matrix[:, 10:13], feature_matrix[:, 14:15]], dim=1)\n",
    "\n",
    "                spectral_list.append(spectral)\n",
    "                temporal_list.append(temporal)\n",
    "\n",
    "            base_name = os.path.basename(file_path)\n",
    "            torch.save(spectral_list, os.path.join(output_dir_spec, base_name))\n",
    "            torch.save(temporal_list, os.path.join(output_dir_temp, base_name))\n",
    "\n",
    "            print(f\"{base_name} in {config} erfolgreich aufgeteilt.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d356698-75b9-4d58-b6a0-c9de013639d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9209f5de-7240-472d-bb43-91c1cbe935cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of top-level object: <class 'list'>\n",
      "Length of list: 1000\n",
      "\n",
      "First element type: <class 'tuple'>\n",
      "  Nested list/tuple with length: 4\n",
      "  First nested element: [[ 0.39671343  0.31002456  1.20167539  0.61168035 -0.78912795  0.56392585\n",
      "   0.78173435  1.22542846  0.65210768 -0.59066113 -0.6107416  -0.66900684\n",
      "   1.06112907  0.45223035 -0.44306134]\n",
      " [ 2.07237451  2.0163187  -0.32710171  1.74845449  1.39890744  1.99441151\n",
      "   1.55668983 -0.17410813  1.66426925  1.62063475  1.59590003  1.03053922\n",
      "  -0.86959759 -0.62985493 -0.53864759]\n",
      " [-0.50184262 -0.04353399  1.41114218 -0.37794431 -0.2044818  -0.45186591\n",
      "   0.36822387  1.20798724 -0.28762193 -0.04417692 -0.29354535  0.64624984\n",
      "  -0.78630999 -0.60705251 -0.53864759]\n",
      " [-0.68282521 -0.50366493 -0.17940142 -0.33128269 -0.86625855 -0.74481138\n",
      "  -0.39366044 -0.02297837 -0.21150563 -0.95220757 -0.90680798 -0.97890649\n",
      "   0.59597551  1.33618937 -0.15656792]\n",
      " [-0.69180106 -0.88276054 -0.73129286 -0.13501362  1.35339536 -0.77654973\n",
      "  -1.14960137 -0.61434612 -0.19227813  1.0116624   1.14405145  1.23338125\n",
      "  -1.25445938 -1.51929399 -0.53864759]\n",
      " [-0.59261905 -0.8963838  -1.37502158 -1.51589421 -0.89243451 -0.58511034\n",
      "  -1.16338623 -1.62198308 -1.62497124 -1.04525153 -0.92885655 -1.26225697\n",
      "   1.25326238  0.96778172  2.21557204]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "file_path = \"montage_datasets/combined/win4_step1/combined_0.pt\"\n",
    "data = torch.load(file_path)\n",
    "\n",
    "print(\"Type of top-level object:\", type(data))\n",
    "print(\"Length of list:\", len(data))\n",
    "\n",
    "# Inspect first element\n",
    "first = data[0]\n",
    "print(\"\\nFirst element type:\", type(first))\n",
    "\n",
    "if isinstance(first, torch.Tensor):\n",
    "    print(\"  Shape:\", first.shape)\n",
    "    print(\"  Dtype:\", first.dtype)\n",
    "    print(\"  Sample values:\", first.flatten()[:5].tolist())\n",
    "\n",
    "elif isinstance(first, list) or isinstance(first, tuple):\n",
    "    print(\"  Nested list/tuple with length:\", len(first))\n",
    "    print(\"  First nested element:\", first[0])\n",
    "\n",
    "else:\n",
    "    print(\"  Value:\", first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ebb5e1-f9eb-445c-9b4c-1675a1d9cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple length: 4\n",
      "\n",
      "Item 0:\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (6, 15)\n",
      "\n",
      "Item 1:\n",
      "  Type: <class 'int'>\n",
      "  Value: 0\n",
      "\n",
      "Item 2:\n",
      "  Type: <class 'str'>\n",
      "  Value: aaaaaaac_s001_t000\n",
      "\n",
      "Item 3:\n",
      "  Type: <class 'float'>\n",
      "  Value: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"montage_datasets/combined/win4_step1/combined_0.pt\")\n",
    "\n",
    "# Print one sample\n",
    "sample = data[0]\n",
    "print(\"Tuple length:\", len(sample))\n",
    "\n",
    "for i, part in enumerate(sample):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(\"  Type:\", type(part))\n",
    "    try:\n",
    "        print(\"  Shape:\", part.shape)\n",
    "    except AttributeError:\n",
    "        print(\"  Value:\", part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead93929-461f-4305-a01b-b9df20600e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'montage_datasets/combined/win4_step1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmontage_datasets/combined/win4_step1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m [sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]   \u001b[38;5;66;03m# Flatten [6,15] → [90]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m [sample[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]             \u001b[38;5;66;03m# Use the int label\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'montage_datasets/combined/win4_step1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"montage_datasets/combined/win4_step1\"\n",
    "data = torch.load(file_path)\n",
    "\n",
    "X = [sample[0].flatten() for sample in data]   # Flatten [6,15] → [90]\n",
    "y = [sample[1] for sample in data]             # Use the int label\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (1000, 90)\n",
    "print(\"y shape:\", y.shape)  # (1000,)\n",
    "print(\"Label distribution:\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f3ea605-fa10-49fd-af4c-1f573af3b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "def compute_scalogram_tensor(window, fs, wavelet='morl', scales=np.arange(1, 64)):\n",
    "    \"\"\"\n",
    "    Compute continuous wavelet transform (scalogram) for each channel.\n",
    "    \"\"\"\n",
    "    scalograms = []\n",
    "    for ch in window:\n",
    "        coeffs, _ = pywt.cwt(ch, scales=scales, wavelet=wavelet, sampling_period=1/fs)\n",
    "        scalogram = np.abs(coeffs)\n",
    "        scalograms.append(scalogram)\n",
    "\n",
    "    scal_tensor = np.stack(scalograms, axis=0)  # (channels, scales, time)\n",
    "    return torch.tensor(scal_tensor, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b91655d1-55aa-4f3a-9d6c-0dfa2cac589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96/6213] ✅ Finished aaaaaqtu_s011_t000 with 297 windows in 1.55sss\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fc6883963a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Process ForkPoolWorker-38337:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-38347:\n",
      "Process ForkPoolWorker-38361:\n",
      "Process ForkPoolWorker-38338:\n",
      "Process ForkPoolWorker-38363:\n",
      "Process ForkPoolWorker-38343:\n",
      "Process ForkPoolWorker-38354:\n",
      "Process ForkPoolWorker-38356:\n",
      "Process ForkPoolWorker-38360:\n",
      "Process ForkPoolWorker-38359:\n",
      "Process ForkPoolWorker-38357:\n",
      "Process ForkPoolWorker-38355:\n",
      "Process ForkPoolWorker-38341:\n",
      "Process ForkPoolWorker-38344:\n",
      "Process ForkPoolWorker-38345:\n",
      "Process ForkPoolWorker-38368:\n",
      "Process ForkPoolWorker-38339:\n",
      "Process ForkPoolWorker-38350:\n",
      "Process ForkPoolWorker-38342:\n",
      "Process ForkPoolWorker-38349:\n",
      "Process ForkPoolWorker-38351:\n",
      "Process ForkPoolWorker-38346:\n",
      "Process ForkPoolWorker-38353:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-38362:\n",
      "Process ForkPoolWorker-38367:\n",
      "Process ForkPoolWorker-38352:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-38348:\n",
      "Process ForkPoolWorker-38358:\n",
      "Process ForkPoolWorker-38365:\n",
      "Process ForkPoolWorker-38364:\n",
      "Process ForkPoolWorker-38366:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-38340:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jupyter-wki_team_3/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     67\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     68\u001b[0m         (idx, window, new_fs, lbl, eeg_id, ts)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (window, lbl, ts) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(windows, labels, timestamps))\n\u001b[1;32m     70\u001b[0m     ]\n\u001b[0;32m---> 71\u001b[0m     pool\u001b[38;5;241m.\u001b[39mmap(compute_single_scalogram, args)\n\u001b[1;32m     73\u001b[0m file_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m file_start\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ✅ Finished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meeg_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(windows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m windows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py:736\u001b[0m, in \u001b[0;36mPool.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py:654\u001b[0m, in \u001b[0;36mPool.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminating pool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[0;32m--> 654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/util.py:224\u001b[0m, in \u001b[0;36mFinalize.__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     sub_debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalizer calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    223\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m--> 224\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weakref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py:692\u001b[0m, in \u001b[0;36mPool._terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    689\u001b[0m task_handler\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[1;32m    691\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelping task handler/workers to finish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_help_stuff_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43minqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m result_handler\u001b[38;5;241m.\u001b[39mis_alive()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(cache) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have cache with result_hander not alive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/multiprocessing/pool.py:672\u001b[0m, in \u001b[0;36mPool._help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_help_stuff_finish\u001b[39m(inqueue, task_handler, size):\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# task_handler may be blocked trying to put items on inqueue\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoving tasks from inqueue until task handler finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 672\u001b[0m     \u001b[43minqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m task_handler\u001b[38;5;241m.\u001b[39mis_alive() \u001b[38;5;129;01mand\u001b[39;00m inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mpoll():\n\u001b[1;32m    674\u001b[0m         inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mrecv()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "import os, torch\n",
    "import numpy as np\n",
    "import pywt\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# === Set config ===\n",
    "window_size = 4\n",
    "step_size = 1\n",
    "save_folder = f\"scalogram_dataset/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# === Parameters shared across processes ===\n",
    "scales = np.linspace(1, 64, 32)\n",
    "wavelet = 'morl'\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "\n",
    "def compute_single_scalogram(args):\n",
    "    \"\"\"Unpack input and compute scalogram.\"\"\"\n",
    "    idx, window, fs, lbl, eeg_id, ts = args\n",
    "    coeffs_all = []\n",
    "    for ch_data in window:\n",
    "        coeffs, _ = pywt.cwt(ch_data, scales, wavelet, sampling_period=1/fs)\n",
    "        coeffs_all.append(np.abs(coeffs).astype(np.float32))\n",
    "    scalogram = np.stack(coeffs_all, axis=0)  # [channels, scales, time]\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "    torch.save((scalogram, lbl, eeg_id, ts), save_path)\n",
    "    return idx\n",
    "\n",
    "\n",
    "# === Main processing loop ===\n",
    "overall_start_time = time.time()\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # 1. Preprocess\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=256, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # 2. Windowing + labeling\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal, resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    if not windows:\n",
    "        print(f\"No valid windows for {eeg_id}\")\n",
    "        continue\n",
    "\n",
    "    # 3. Generate scalograms in parallel\n",
    "    file_start = time.time()\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        args = [\n",
    "            (idx, window, new_fs, lbl, eeg_id, ts)\n",
    "            for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps))\n",
    "        ]\n",
    "        pool.map(compute_single_scalogram, args)\n",
    "\n",
    "    file_time = time.time() - file_start\n",
    "    print(f\"[{i+1}/{len(dataset)}] ✅ Finished {eeg_id} with {len(windows)} windows in {file_time:.2f}s\", end = '\\r')\n",
    "\n",
    "total_time = time.time() - overall_start_time\n",
    "print(f\"\\n⏱️ Total time to generate scalograms: {total_time:.2f} seconds\")\n",
    "print(f\"✅ All done. Saved to: {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2435fa5b-f3bb-4cb9-a920-d9bd1d89138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Folder\n",
      "[6213/6213] ✅ aaaaatez_s006_t010 with 297 windows in 0.15s8ss\n",
      "⏱️ Total time: 2501.54 seconds\n",
      "✅ Finished preprocessing to: raw_dataset/win4_step1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "window_size = 4  # seconds\n",
    "step_size = 1    # seconds\n",
    "target_fs = 256  # Hz\n",
    "save_folder = f\"raw_dataset/win{window_size}_step{step_size}\"\n",
    "print(\"Creating Folder\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# === Dataset ===\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "# === Timing (optional) ===\n",
    "overall_start = time.time()\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # === Preprocess with Montages ===\n",
    "    #print(f\"Start Preprocessing: {eeg_id}\", end ='\\r')\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=target_fs, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # === Window and Label ===\n",
    "    #print(f\"Start Windowing: {eeg_id}\", end ='\\r')\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal,\n",
    "        resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    file_start = time.time()\n",
    "    for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps)):\n",
    "\n",
    "        # Optional: Normalize each channel in the window (z-score)\n",
    "        window = (window - window.mean(axis=1, keepdims=True)) / (window.std(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "        save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "        torch.save((window, lbl, eeg_id, ts), save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] ✅ {eeg_id} with {len(windows)} windows in {time.time() - file_start:.2f}s\", end='\\r')\n",
    "\n",
    "# === Overall Time ===\n",
    "overall_time = time.time() - overall_start\n",
    "print(f\"\\n⏱️ Total time: {overall_time:.2f} seconds\")\n",
    "print(f\"✅ Finished preprocessing to: {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8c44d-2e44-4590-8b51-62343c04a646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
