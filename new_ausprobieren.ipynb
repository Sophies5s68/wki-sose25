{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f385f6a-f913-40a9-a0d0-0b50fbe42dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ich probier jetzt jeden Schritt nochmal von vorne und schau mir den Schritt nochmal genauer an'\n",
    "\n",
    "from wettbewerb import load_references, get_6montages\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "\n",
    "\n",
    "ids, channels_list, data_list, fs_list, ref_list, label_list = load_references(folder=\"../shared_data/training\", idx=0)\n",
    "for i in range(100):\n",
    "    #montage_names, montage_data, missing = get_6montages(channels_list[i], data_list[i])\n",
    "    #print(f\"{ids[i]}:{montage_names}:{montage_data.shape}\\n {fs_list[i]}\")\n",
    "    #if missing:\n",
    "        #print(\"Warning: Montage missing, data may be incomplete.\")\n",
    "    #print (f\"{ids[i]}\",label_list[i])\n",
    "    processed_signal, montage_missing = preprocess_signal_with_montages(channels_list[i], data_list[i], 256, fs_list[i])\n",
    "    print(f\"{ids[i]}:\",processed_signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdc1a4-54fe-4817-b565-0d19f94762ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "labels = dataset.get_labels()\n",
    "\n",
    "# Count seizure / non-seizure\n",
    "seizure_count = sum(1 for l in labels if l[0])\n",
    "non_seizure_count = len(labels) - seizure_count\n",
    "\n",
    "print(f\"Total: {len(labels)}\")\n",
    "print(f\"Seizures: {seizure_count}\")\n",
    "print(f\"Non-Seizures: {non_seizure_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d046948-7f78-4882-81c4-19a37e08f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "import os\n",
    "import torch \n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = \"preprocessed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for i in range(total):\n",
    "    ids, channels, data, fs, ref, label = dataset[i]\n",
    "\n",
    "    montage_names, processed_signal, montage_missing, resampled_fs = preprocess_signal_with_montages(\n",
    "        channels, data, target_fs=256, original_fs=fs, ids=ids\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        skipped_count += 1\n",
    "        print(f\"[{i+1}/{total}] Skipping {ids} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{ids}.pt\")\n",
    "    torch.save((processed_signal, label, ids, montage_names, resampled_fs), save_path)\n",
    "    processed_count += 1\n",
    "    print(f\"[{i+1}/{total}] Processed: {processed_count} | Skipped: {skipped_count}\", end='\\r')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0c4a3-d55d-4088-a3b1-ed0ef2c21959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data, feature_extraction_window  # your modules\n",
    "import os, torch\n",
    "import numpy as np\n",
    "\n",
    "window_size = 4  # seconds\n",
    "step_size = 2    # seconds\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = f\"montage_datasets/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # 1. Preprocess\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=256, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # 2. Windowing + labeling\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal, resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    # 3. Feature extraction per window\n",
    "    for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps)):\n",
    "        features = feature_extraction_window(window, new_fs)\n",
    "        save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "        torch.save((features, lbl, eeg_id, ts), save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] Processed {eeg_id} with {len(windows)} windows.\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ae54-6c8b-4551-82d9-6e7ec1c6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen der Features in zeitliche und spektrale\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "def split_features(feature_tensor):\n",
    "    \"\"\"\n",
    "    Trennt Features in spektral (0-9) und zeitlich (10-14)\n",
    "    \"\"\"\n",
    "    spectral = feature_tensor[..., :10]         # Indizes 0-9\n",
    "    temporal = feature_tensor[..., 10:15]       # Indizes 10-14\n",
    "    return spectral, temporal\n",
    "\n",
    "def process_feature_files(load_dir, save_dir_spectral, save_dir_temporal):\n",
    "    os.makedirs(save_dir_spectral, exist_ok=True)\n",
    "    os.makedirs(save_dir_temporal, exist_ok=True)\n",
    "\n",
    "    feature_files = glob(os.path.join(load_dir, \"*.pt\"))\n",
    "\n",
    "    for file in feature_files:\n",
    "        data = torch.load(file)\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            features, label, eeg_id, ts = data\n",
    "        elif isinstance(data, dict):\n",
    "            features = data['features']\n",
    "            label = data['label']\n",
    "            eeg_id = data['eeg_id']\n",
    "            ts = data['timestamp']\n",
    "        else:\n",
    "            print(f\"Unbekanntes Format: {file}\")\n",
    "            continue\n",
    "\n",
    "        # in numpy falls tensor\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = features.numpy()\n",
    "\n",
    "        # flach oder Matrix?\n",
    "        if features.ndim == 1:\n",
    "            n_channels = 6  # oder dein tatsächlicher Wert\n",
    "            features = features.reshape(n_channels, -1)\n",
    "\n",
    "        spec_feat, time_feat = split_features(features)\n",
    "\n",
    "        # Optional: flatten\n",
    "        spec_feat_flat = spec_feat.flatten()\n",
    "        time_feat_flat = time_feat.flatten()\n",
    "\n",
    "        base_name = os.path.basename(file)\n",
    "\n",
    "        # Speichern\n",
    "        torch.save((spec_feat_flat, label, eeg_id, ts), os.path.join(save_dir_spectral, base_name))\n",
    "        torch.save((time_feat_flat, label, eeg_id, ts), os.path.join(save_dir_temporal, base_name))\n",
    "\n",
    "    print(f\"Fertig. {len(feature_files)} Dateien verarbeitet.\")\n",
    "\n",
    "# Beispiel:\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "    \n",
    "    load_dir = \"montage_datasets/\"+ config\n",
    "    save_dir_spectral = \"data_features_sep/spectral/\" + config\n",
    "    save_dir_temporal = \"data_features_sep/temporal/\" + config\n",
    "\n",
    "    process_feature_files(load_dir, save_dir_spectral, save_dir_temporal)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da117929-d02f-4838-b337-bcb0150ccbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code zum Zusammenführen von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"montage_datasets/\" + config\n",
    "    target_dir = \"montage_datasets/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc7a116-ca5b-43f6-b518-62d0eebd9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sps\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model_copy import CNN_EEG\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from features_prediction import window_prediction, feature_extraction_window\n",
    "#from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "from scipy.signal import iirnotch, butter, sosfiltfilt, resample_poly, tf2sos\n",
    "\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht verändert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der übergebenen Kanäle\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kanäle\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enthält Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bezüglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bezüglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4\n",
    "    step_size = 1\n",
    "    target_fs = 256\n",
    "    original_fs = fs\n",
    " \n",
    "    \n",
    "    montage_names, montage_data, montage_missing,target_fs = preprocess_signal_with_montages(channels, data,target_fs,original_fs) \n",
    "    \n",
    "    windows, timestamps = window_prediction(montage_data, target_fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction_window(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        x = torch.tensor(features, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        prob = predictions_ensemble(data_for_class ,model_name, device)\n",
    "        predictions_per_window = [int(p > 0.5) for p in probs]\n",
    "\n",
    "    seizure_present = False\n",
    "    seizure_present, onset_candidate = detect_onset(predictions_per_window, timestamps, min_consecutive=2)\n",
    "    if seizure_present:\n",
    "        onset = onset_candidate\n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unverändert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(data_for_class: List[torch.Tensor], model_name: str, device: torch.device) -> List[float]:\n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "    batch_tensor = torch.stack(data_for_class).to(device)\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = CNN_EEG(6, 1).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "            outputs = torch.sigmoid(model(batch_tensor).squeeze())\n",
    "            probs.append(outputs.cpu().numpy())  # shape: (num_windows,)\n",
    "\n",
    "    ensemble_probs = np.mean(probs, axis=0)  # Mittelwert pro Fenster\n",
    "    return ensemble_probs.tolist()  # Gib Liste von Wahrscheinlichkeiten zurück\n",
    "\n",
    "\n",
    "def detect_onset(predictions, timestamps, min_consecutive=2):\n",
    "    predictions = torch.tensor(predictions)\n",
    "    for i in range(len(predictions) - min_consecutive + 1):\n",
    "        if torch.all(predictions[i:i+min_consecutive] == 1):\n",
    "            return True, timestamps[i]\n",
    "    return False, None\n",
    "\n",
    "\n",
    "\n",
    "def notch_filter(signal, fs, freq=50.0, Q=30.0):\n",
    "    w0 = freq / (fs / 2)\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    sos = tf2sos(b, a)  # Transferfunktion → SOS\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "\n",
    "def bandpass_filter(signal, fs, lowcut=1.0, highcut=120.0, order=4):\n",
    "    sos = sps.butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs=256):\n",
    "    if original_fs == target_fs:\n",
    "        return signal\n",
    "    gcd = np.gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // gcd)\n",
    "    down = int(original_fs // gcd)\n",
    "    return resample_poly(signal, up, down, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc420f0-3c75-4a50-977a-5a233789ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t Dateien wurden geladen.\n",
      "[(1, 26.08, 50.1025), (1, 2.9212, 32.3607), (0, 0.0, 0.0), (1, 34.8275, 63.0425), (1, 9.1971, 23.5505), (1, 5.285, 26.4575), (1, 19.7015, 28.5202), (0, 0.0, 0.0), (0, 0.0, 0.0), (0, 0.0, 0.0)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m ref \u001b[38;5;241m=\u001b[39m reference_systems[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     10\u001b[0m model_abgabe \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_abgabe/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_abgabe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "Cell \u001b[0;32mIn[7], line 90\u001b[0m, in \u001b[0;36mpredict_labels\u001b[0;34m(channels, data, fs, reference_system, model_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     89\u001b[0m     prob \u001b[38;5;241m=\u001b[39m predictions_ensemble(data_for_class ,model_name, device)\n\u001b[0;32m---> 90\u001b[0m     predictions_per_window \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprobs\u001b[49m]\n\u001b[1;32m     92\u001b[0m seizure_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     93\u001b[0m seizure_present, onset_candidate \u001b[38;5;241m=\u001b[39m detect_onset(predictions_per_window, timestamps, min_consecutive\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "from wettbewerb import load_references\n",
    "train_folder = \"../shared_data/training_mini\" \n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder,90)\n",
    "print(eeg_labels)\n",
    "idx = ids[3]\n",
    "channel = channels[3]\n",
    "data_s = data[3]\n",
    "fs = sampling_frequencies[3]\n",
    "ref = reference_systems[3]\n",
    "model_abgabe = \"model_abgabe/\"\n",
    "prediction = predict_labels(channel, data_s, fs, ref, model_abgabe)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1298fb-03e9-455a-b338-45e7370e768f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495f84f-0191-4e11-b654-09b96388d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code zum Zusammenführen von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"montage_datasets/\" + config\n",
    "    target_dir = \"montage_datasets/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310810f-d1ec-4b7a-94f6-1ee0ba545462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "# === Einstellungen ===\n",
    "input_root = \"montage_datasets/combined/\"\n",
    "output_root_spectral = \"montage_datasets/spectral_only/\"\n",
    "output_root_temporal = \"montage_datasets/temporal_only/\"\n",
    "\n",
    "# Erstelle Zielverzeichnisse, wenn nicht vorhanden\n",
    "os.makedirs(output_root_spectral, exist_ok=True)\n",
    "os.makedirs(output_root_temporal, exist_ok=True)\n",
    "\n",
    "# Alle Konfigurations-Unterordner finden\n",
    "configs = [f for f in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, f))]\n",
    "\n",
    "for config in configs:\n",
    "    input_dir = os.path.join(input_root, config)\n",
    "    output_dir_spec = os.path.join(output_root_spectral, config)\n",
    "    output_dir_temp = os.path.join(output_root_temporal, config)\n",
    "\n",
    "    os.makedirs(output_dir_spec, exist_ok=True)\n",
    "    os.makedirs(output_dir_temp, exist_ok=True)\n",
    "\n",
    "    pt_files = sorted(glob(os.path.join(input_dir, \"*.pt\")))\n",
    "\n",
    "    for file_path in pt_files:\n",
    "        try:\n",
    "            samples = torch.load(file_path)  # List of (channels x 15) matrices\n",
    "\n",
    "            spectral_list = []\n",
    "            temporal_list = []\n",
    "\n",
    "            for sample in samples:\n",
    "                feature_matrix = sample[0]  # (channels x 15)\n",
    "                if isinstance(feature_matrix, np.ndarray):\n",
    "                    feature_matrix = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "                print(f\"feature_matrix shape: {feature_matrix.shape}, dtype: {feature_matrix.dtype}\")\n",
    "                spectral = torch.cat([feature_matrix[:, :10], feature_matrix[:, 13:14]], dim=1)\n",
    "                temporal = torch.cat([feature_matrix[:, 10:13], feature_matrix[:, 14:15]], dim=1)\n",
    "\n",
    "                spectral_list.append(spectral)\n",
    "                temporal_list.append(temporal)\n",
    "\n",
    "            base_name = os.path.basename(file_path)\n",
    "            torch.save(spectral_list, os.path.join(output_dir_spec, base_name))\n",
    "            torch.save(temporal_list, os.path.join(output_dir_temp, base_name))\n",
    "\n",
    "            print(f\"{base_name} in {config} erfolgreich aufgeteilt.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d356698-75b9-4d58-b6a0-c9de013639d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
