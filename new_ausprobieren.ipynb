{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f385f6a-f913-40a9-a0d0-0b50fbe42dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ich probier jetzt jeden Schritt nochmal von vorne und schau mir den Schritt nochmal genauer an'\n",
    "\n",
    "from wettbewerb import load_references, get_6montages\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "\n",
    "\n",
    "ids, channels_list, data_list, fs_list, ref_list, label_list = load_references(folder=\"../shared_data/training\", idx=0)\n",
    "for i in range(100):\n",
    "    #montage_names, montage_data, missing = get_6montages(channels_list[i], data_list[i])\n",
    "    #print(f\"{ids[i]}:{montage_names}:{montage_data.shape}\\n {fs_list[i]}\")\n",
    "    #if missing:\n",
    "        #print(\"Warning: Montage missing, data may be incomplete.\")\n",
    "    #print (f\"{ids[i]}\",label_list[i])\n",
    "    processed_signal, montage_missing = preprocess_signal_with_montages(channels_list[i], data_list[i], 256, fs_list[i])\n",
    "    print(f\"{ids[i]}:\",processed_signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdc1a4-54fe-4817-b565-0d19f94762ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "labels = dataset.get_labels()\n",
    "\n",
    "# Count seizure / non-seizure\n",
    "seizure_count = sum(1 for l in labels if l[0])\n",
    "non_seizure_count = len(labels) - seizure_count\n",
    "\n",
    "print(f\"Total: {len(labels)}\")\n",
    "print(f\"Seizures: {seizure_count}\")\n",
    "print(f\"Non-Seizures: {non_seizure_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d046948-7f78-4882-81c4-19a37e08f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "import os\n",
    "import torch \n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = \"preprocessed_data\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for i in range(total):\n",
    "    ids, channels, data, fs, ref, label = dataset[i]\n",
    "\n",
    "    montage_names, processed_signal, montage_missing, resampled_fs = preprocess_signal_with_montages(\n",
    "        channels, data, target_fs=256, original_fs=fs, ids=ids\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        skipped_count += 1\n",
    "        print(f\"[{i+1}/{total}] Skipping {ids} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{ids}.pt\")\n",
    "    torch.save((processed_signal, label, ids, montage_names, resampled_fs), save_path)\n",
    "    processed_count += 1\n",
    "    print(f\"[{i+1}/{total}] Processed: {processed_count} | Skipped: {skipped_count}\", end='\\r')\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0c4a3-d55d-4088-a3b1-ed0ef2c21959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data, feature_extraction_window  # your modules\n",
    "import os, torch\n",
    "import numpy as np\n",
    "\n",
    "window_size = 4  # seconds\n",
    "step_size = 2    # seconds\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "save_folder = f\"montage_datasets/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # 1. Preprocess\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=256, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # 2. Windowing + labeling\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal, resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    # 3. Feature extraction per window\n",
    "    for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps)):\n",
    "        features = feature_extraction_window(window, new_fs)\n",
    "        save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "        torch.save((features, lbl, eeg_id, ts), save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] Processed {eeg_id} with {len(windows)} windows.\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600ae54-6c8b-4551-82d9-6e7ec1c6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen der Features in zeitliche und spektrale\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "def split_features(feature_tensor):\n",
    "    \"\"\"\n",
    "    Trennt Features in spektral (0-9) und zeitlich (10-14)\n",
    "    \"\"\"\n",
    "    spectral = feature_tensor[..., :10]         # Indizes 0-9\n",
    "    temporal = feature_tensor[..., 10:15]       # Indizes 10-14\n",
    "    return spectral, temporal\n",
    "\n",
    "def process_feature_files(load_dir, save_dir_spectral, save_dir_temporal):\n",
    "    os.makedirs(save_dir_spectral, exist_ok=True)\n",
    "    os.makedirs(save_dir_temporal, exist_ok=True)\n",
    "\n",
    "    feature_files = glob(os.path.join(load_dir, \"*.pt\"))\n",
    "\n",
    "    for file in feature_files:\n",
    "        data = torch.load(file)\n",
    "\n",
    "        if isinstance(data, tuple):\n",
    "            features, label, eeg_id, ts = data\n",
    "        elif isinstance(data, dict):\n",
    "            features = data['features']\n",
    "            label = data['label']\n",
    "            eeg_id = data['eeg_id']\n",
    "            ts = data['timestamp']\n",
    "        else:\n",
    "            print(f\"Unbekanntes Format: {file}\")\n",
    "            continue\n",
    "\n",
    "        # in numpy falls tensor\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = features.numpy()\n",
    "\n",
    "        # flach oder Matrix?\n",
    "        if features.ndim == 1:\n",
    "            n_channels = 6  # oder dein tats√§chlicher Wert\n",
    "            features = features.reshape(n_channels, -1)\n",
    "\n",
    "        spec_feat, time_feat = split_features(features)\n",
    "\n",
    "        # Optional: flatten\n",
    "        spec_feat_flat = spec_feat.flatten()\n",
    "        time_feat_flat = time_feat.flatten()\n",
    "\n",
    "        base_name = os.path.basename(file)\n",
    "\n",
    "        # Speichern\n",
    "        torch.save((spec_feat_flat, label, eeg_id, ts), os.path.join(save_dir_spectral, base_name))\n",
    "        torch.save((time_feat_flat, label, eeg_id, ts), os.path.join(save_dir_temporal, base_name))\n",
    "\n",
    "    print(f\"Fertig. {len(feature_files)} Dateien verarbeitet.\")\n",
    "\n",
    "# Beispiel:\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "    \n",
    "    load_dir = \"montage_datasets/\"+ config\n",
    "    save_dir_spectral = \"data_features_sep/spectral/\" + config\n",
    "    save_dir_temporal = \"data_features_sep/temporal/\" + config\n",
    "\n",
    "    process_feature_files(load_dir, save_dir_spectral, save_dir_temporal)\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da117929-d02f-4838-b337-bcb0150ccbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config win4_step1 gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Code zum Zusammenf√ºhren von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/add_dataset/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"add_dataset/\" + config\n",
    "    target_dir = \"add_dataset/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7a116-ca5b-43f6-b518-62d0eebd9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# VERSION HAT FUNKTIONIERT\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sps\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model_copy import CNN_EEG\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from features_prediction import window_prediction, feature_extraction_window\n",
    "#from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "from scipy.signal import iirnotch, butter, sosfiltfilt, resample_poly, tf2sos\n",
    "\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht ver√§ndert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der √ºbergebenen Kan√§le\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kan√§le\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enth√§lt Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bez√ºglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bez√ºglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4\n",
    "    step_size = 1\n",
    "    target_fs = 256\n",
    "    original_fs = fs\n",
    " \n",
    "    \n",
    "    montage_names, montage_data, montage_missing,target_fs = preprocess_signal_with_montages(channels, data,target_fs,original_fs) \n",
    "    \n",
    "    windows, timestamps = window_prediction(montage_data, target_fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction_window(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        x = torch.tensor(features, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        probs = predictions_ensemble(data_for_class ,model_name, device)\n",
    "        predictions_per_window = [int(p > 0.5) for p in probs]\n",
    "\n",
    "    seizure_present = False\n",
    "    seizure_present, onset_candidate = detect_onset(predictions_per_window, timestamps, min_consecutive=2)\n",
    "    if seizure_present:\n",
    "        onset = onset_candidate\n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unver√§ndert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(data_for_class: List[torch.Tensor], model_name: str, device: torch.device) -> List[float]:\n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "    batch_tensor = torch.stack(data_for_class).to(device)\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = CNN_EEG(6, 1).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "            outputs = torch.sigmoid(model(batch_tensor).squeeze())\n",
    "            probs.append(outputs.cpu().numpy())  # shape: (num_windows,)\n",
    "\n",
    "    ensemble_probs = np.mean(probs, axis=0)  # Mittelwert pro Fenster\n",
    "    return ensemble_probs.tolist()  # Gib Liste von Wahrscheinlichkeiten zur√ºck\n",
    "\n",
    "\n",
    "def detect_onset(predictions, timestamps, min_consecutive=2):\n",
    "    predictions = torch.tensor(predictions)\n",
    "    for i in range(len(predictions) - min_consecutive + 1):\n",
    "        if torch.all(predictions[i:i+min_consecutive] == 1):\n",
    "            return True, timestamps[i]\n",
    "    return False, None\n",
    "\n",
    "\n",
    "\n",
    "def notch_filter(signal, fs, freq=50.0, Q=30.0):\n",
    "    w0 = freq / (fs / 2)\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    sos = tf2sos(b, a)  # Transferfunktion ‚Üí SOS\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "\n",
    "def bandpass_filter(signal, fs, lowcut=1.0, highcut=120.0, order=4):\n",
    "    sos = sps.butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs=256):\n",
    "    if original_fs == target_fs:\n",
    "        return signal\n",
    "    gcd = np.gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // gcd)\n",
    "    down = int(original_fs // gcd)\n",
    "    return resample_poly(signal, up, down, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc420f0-3c75-4a50-977a-5a233789ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references\n",
    "train_folder = \"../shared_data/training_mini\" \n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder,90)\n",
    "print(eeg_labels)\n",
    "idx = ids[3]\n",
    "channel = channels[3]\n",
    "data_s = data[3]\n",
    "fs = sampling_frequencies[3]\n",
    "ref = reference_systems[3]\n",
    "model_abgabe = \"model_abgabe/\"\n",
    "prediction = predict_labels(channel, data_s, fs, ref, model_abgabe)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1298fb-03e9-455a-b338-45e7370e768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Skript testet das vortrainierte Modell\n",
    "\n",
    "\n",
    "@author:  Maurice Rohr, Dirk Schweickard\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sps\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model_copy import CNN_EEG\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from faster_features import window_prediction, feature_extraction_window\n",
    "#from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "from scipy.signal import iirnotch, butter, sosfiltfilt, resample_poly, tf2sos\n",
    "\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht ver√§ndert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der √ºbergebenen Kan√§le\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kan√§le\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enth√§lt Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bez√ºglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bez√ºglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4\n",
    "    step_size = 1\n",
    "    target_fs = 256\n",
    "    original_fs = fs\n",
    " \n",
    "    \n",
    "    montage_names, montage_data, montage_missing,target_fs = preprocess_signal_with_montages(channels, data,target_fs,original_fs) \n",
    "    \n",
    "    windows, timestamps = window_prediction(montage_data, target_fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction_window(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        x = torch.tensor(features, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        probs = predictions_ensemble(data_for_class ,model_name, device)\n",
    "        predictions_per_window = [int(p > 0.5) for p in probs]\n",
    "\n",
    "    seizure_present = False\n",
    "    seizure_present, onset_candidate = detect_onset(predictions_per_window, timestamps, min_consecutive=2)\n",
    "    if seizure_present:\n",
    "        onset = onset_candidate\n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unver√§ndert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(data_for_class: List[torch.Tensor], model_name: str, device: torch.device) -> List[float]:\n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "    batch_tensor = torch.stack(data_for_class).to(device)\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = CNN_EEG(6, 1).to(device)\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.eval()\n",
    "            outputs = torch.sigmoid(model(batch_tensor).squeeze())\n",
    "            probs.append(outputs.cpu().numpy())  # shape: (num_windows,)\n",
    "\n",
    "    ensemble_probs = np.mean(probs, axis=0)  # Mittelwert pro Fenster\n",
    "    return ensemble_probs.tolist()  # Gib Liste von Wahrscheinlichkeiten zur√ºck\n",
    "\n",
    "\n",
    "def detect_onset(predictions, timestamps, min_consecutive=2):\n",
    "    predictions = torch.tensor(predictions)\n",
    "    for i in range(len(predictions) - min_consecutive + 1):\n",
    "        if torch.all(predictions[i:i+min_consecutive] == 1):\n",
    "            return True, timestamps[i]\n",
    "    return False, None\n",
    "\n",
    "\n",
    "\n",
    "def notch_filter(signal, fs, freq=50.0, Q=30.0):\n",
    "    w0 = freq / (fs / 2)\n",
    "    b, a = iirnotch(w0, Q)\n",
    "    sos = tf2sos(b, a)  # Transferfunktion ‚Üí SOS\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "\n",
    "def bandpass_filter(signal, fs, lowcut=1.0, highcut=120.0, order=4):\n",
    "    sos = sps.butter(order, [lowcut, highcut], btype='band', fs=fs, output='sos')\n",
    "    return sosfiltfilt(sos, signal, axis=-1)\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs=256):\n",
    "    if original_fs == target_fs:\n",
    "        return signal\n",
    "    gcd = np.gcd(int(original_fs), int(target_fs))\n",
    "    up = int(target_fs // gcd)\n",
    "    down = int(original_fs // gcd)\n",
    "    return resample_poly(signal, up, down, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6697f-0e98-4f87-9e8a-2a98005a0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references\n",
    "train_folder = \"../shared_data/training_mini\" \n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder,90)\n",
    "print(eeg_labels)\n",
    "idx = ids[3]\n",
    "channel = channels[3]\n",
    "data_s = data[3]\n",
    "fs = sampling_frequencies[3]\n",
    "ref = reference_systems[3]\n",
    "model_abgabe = \"model_abgabe/\"\n",
    "prediction = predict_labels(channel, data_s, fs, ref, model_abgabe)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495f84f-0191-4e11-b654-09b96388d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code zum Zusammenf√ºhren von .pt Dateien -> reduziert die LAdezeit am Anfang des Traiings massiv\n",
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "ordner = \"/home/jupyter-wki_team_3/wki-sose25/montage_datasets/\"\n",
    "unterordner = [f for f in os.listdir(ordner) if os.path.isdir(os.path.join(ordner, f)) and not f.startswith('.')]\n",
    "    \n",
    "for config in unterordner:\n",
    "\n",
    "    # === Einstellungen ===\n",
    "    source_dir = \"montage_datasets/\" + config\n",
    "    target_dir = \"montage_datasets/combined/\" + config\n",
    "    batch_size = 1000  # Anzahl Dateien pro kombiniertes File\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # === Alle .pt-Dateien finden ===\n",
    "    file_paths = sorted(glob(os.path.join(source_dir, \"*.pt\")))\n",
    "\n",
    "    combined_samples = []\n",
    "    file_counter = 0\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            sample = torch.load(file_path)\n",
    "            combined_samples.append(sample)\n",
    "\n",
    "            # Sobald batch_size erreicht ist, speichern\n",
    "            if len(combined_samples) >= batch_size:\n",
    "                save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "                torch.save(combined_samples, save_path)\n",
    "                combined_samples = []\n",
    "                file_counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")\n",
    "\n",
    "    # Rest speichern\n",
    "    if combined_samples:\n",
    "        save_path = os.path.join(target_dir, f\"combined_{file_counter}.pt\")\n",
    "        torch.save(combined_samples, save_path)\n",
    "        \n",
    "\n",
    "    print(f\"config {config} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310810f-d1ec-4b7a-94f6-1ee0ba545462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "# === Einstellungen ===\n",
    "input_root = \"montage_datasets/combined/\"\n",
    "output_root_spectral = \"montage_datasets/spectral_only/\"\n",
    "output_root_temporal = \"montage_datasets/temporal_only/\"\n",
    "\n",
    "# Erstelle Zielverzeichnisse, wenn nicht vorhanden\n",
    "os.makedirs(output_root_spectral, exist_ok=True)\n",
    "os.makedirs(output_root_temporal, exist_ok=True)\n",
    "\n",
    "# Alle Konfigurations-Unterordner finden\n",
    "configs = [f for f in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, f))]\n",
    "\n",
    "for config in configs:\n",
    "    input_dir = os.path.join(input_root, config)\n",
    "    output_dir_spec = os.path.join(output_root_spectral, config)\n",
    "    output_dir_temp = os.path.join(output_root_temporal, config)\n",
    "\n",
    "    os.makedirs(output_dir_spec, exist_ok=True)\n",
    "    os.makedirs(output_dir_temp, exist_ok=True)\n",
    "\n",
    "    pt_files = sorted(glob(os.path.join(input_dir, \"*.pt\")))\n",
    "\n",
    "    for file_path in pt_files:\n",
    "        try:\n",
    "            samples = torch.load(file_path)  # List of (channels x 15) matrices\n",
    "\n",
    "            spectral_list = []\n",
    "            temporal_list = []\n",
    "\n",
    "            for sample in samples:\n",
    "                feature_matrix = sample[0]  # (channels x 15)\n",
    "                if isinstance(feature_matrix, np.ndarray):\n",
    "                    feature_matrix = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "                print(f\"feature_matrix shape: {feature_matrix.shape}, dtype: {feature_matrix.dtype}\")\n",
    "                spectral = torch.cat([feature_matrix[:, :10], feature_matrix[:, 13:14]], dim=1)\n",
    "                temporal = torch.cat([feature_matrix[:, 10:13], feature_matrix[:, 14:15]], dim=1)\n",
    "\n",
    "                spectral_list.append(spectral)\n",
    "                temporal_list.append(temporal)\n",
    "\n",
    "            base_name = os.path.basename(file_path)\n",
    "            torch.save(spectral_list, os.path.join(output_dir_spec, base_name))\n",
    "            torch.save(temporal_list, os.path.join(output_dir_temp, base_name))\n",
    "\n",
    "            print(f\"{base_name} in {config} erfolgreich aufgeteilt.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d356698-75b9-4d58-b6a0-c9de013639d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209f5de-7240-472d-bb43-91c1cbe935cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "file_path = \"montage_datasets/combined/win4_step1/combined_0.pt\"\n",
    "data = torch.load(file_path)\n",
    "\n",
    "print(\"Type of top-level object:\", type(data))\n",
    "print(\"Length of list:\", len(data))\n",
    "\n",
    "# Inspect first element\n",
    "first = data[0]\n",
    "print(\"\\nFirst element type:\", type(first))\n",
    "\n",
    "if isinstance(first, torch.Tensor):\n",
    "    print(\"  Shape:\", first.shape)\n",
    "    print(\"  Dtype:\", first.dtype)\n",
    "    print(\"  Sample values:\", first.flatten()[:5].tolist())\n",
    "\n",
    "elif isinstance(first, list) or isinstance(first, tuple):\n",
    "    print(\"  Nested list/tuple with length:\", len(first))\n",
    "    print(\"  First nested element:\", first[0])\n",
    "\n",
    "else:\n",
    "    print(\"  Value:\", first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebb5e1-f9eb-445c-9b4c-1675a1d9cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"montage_datasets/combined/win4_step1/combined_0.pt\")\n",
    "\n",
    "# Print one sample\n",
    "sample = data[0]\n",
    "print(\"Tuple length:\", len(sample))\n",
    "\n",
    "for i, part in enumerate(sample):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(\"  Type:\", type(part))\n",
    "    try:\n",
    "        print(\"  Shape:\", part.shape)\n",
    "    except AttributeError:\n",
    "        print(\"  Value:\", part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead93929-461f-4305-a01b-b9df20600e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"montage_datasets/combined/win4_step1\"\n",
    "data = torch.load(file_path)\n",
    "\n",
    "X = [sample[0].flatten() for sample in data]   # Flatten [6,15] ‚Üí [90]\n",
    "y = [sample[1] for sample in data]             # Use the int label\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (1000, 90)\n",
    "print(\"y shape:\", y.shape)  # (1000,)\n",
    "print(\"Label distribution:\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ea605-fa10-49fd-af4c-1f573af3b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "def compute_scalogram_tensor(window, fs, wavelet='morl', scales=np.arange(1, 64)):\n",
    "    \"\"\"\n",
    "    Compute continuous wavelet transform (scalogram) for each channel.\n",
    "    \"\"\"\n",
    "    scalograms = []\n",
    "    for ch in window:\n",
    "        coeffs, _ = pywt.cwt(ch, scales=scales, wavelet=wavelet, sampling_period=1/fs)\n",
    "        scalogram = np.abs(coeffs)\n",
    "        scalograms.append(scalogram)\n",
    "\n",
    "    scal_tensor = np.stack(scalograms, axis=0)  # (channels, scales, time)\n",
    "    return torch.tensor(scal_tensor, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91655d1-55aa-4f3a-9d6c-0dfa2cac589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "import os, torch\n",
    "import numpy as np\n",
    "import pywt\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# === Set config ===\n",
    "window_size = 4\n",
    "step_size = 1\n",
    "save_folder = f\"scalogram_dataset/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# === Parameters shared across processes ===\n",
    "scales = np.linspace(1, 64, 32)\n",
    "wavelet = 'morl'\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "\n",
    "def compute_single_scalogram(args):\n",
    "    \"\"\"Unpack input and compute scalogram.\"\"\"\n",
    "    idx, window, fs, lbl, eeg_id, ts = args\n",
    "    coeffs_all = []\n",
    "    for ch_data in window:\n",
    "        coeffs, _ = pywt.cwt(ch_data, scales, wavelet, sampling_period=1/fs)\n",
    "        coeffs_all.append(np.abs(coeffs).astype(np.float32))\n",
    "    scalogram = np.stack(coeffs_all, axis=0)  # [channels, scales, time]\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "    torch.save((scalogram, lbl, eeg_id, ts), save_path)\n",
    "    return idx\n",
    "\n",
    "\n",
    "# === Main processing loop ===\n",
    "overall_start_time = time.time()\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # 1. Preprocess\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=256, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # 2. Windowing + labeling\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal, resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    if not windows:\n",
    "        print(f\"No valid windows for {eeg_id}\")\n",
    "        continue\n",
    "\n",
    "    # 3. Generate scalograms in parallel\n",
    "    file_start = time.time()\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        args = [\n",
    "            (idx, window, new_fs, lbl, eeg_id, ts)\n",
    "            for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps))\n",
    "        ]\n",
    "        pool.map(compute_single_scalogram, args)\n",
    "\n",
    "    file_time = time.time() - file_start\n",
    "    print(f\"[{i+1}/{len(dataset)}] ‚úÖ Finished {eeg_id} with {len(windows)} windows in {file_time:.2f}s\", end = '\\r')\n",
    "\n",
    "total_time = time.time() - overall_start_time\n",
    "print(f\"\\n‚è±Ô∏è Total time to generate scalograms: {total_time:.2f} seconds\")\n",
    "print(f\"‚úÖ All done. Saved to: {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435fa5b-f3bb-4cb9-a920-d9bd1d89138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "window_size = 4  # seconds\n",
    "step_size = 1    # seconds\n",
    "target_fs = 256  # Hz\n",
    "save_folder = f\"raw_dataset/win{window_size}_step{step_size}\"\n",
    "print(\"Creating Folder\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# === Dataset ===\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "# === Timing (optional) ===\n",
    "overall_start = time.time()\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # === Preprocess with Montages ===\n",
    "    #print(f\"Start Preprocessing: {eeg_id}\", end ='\\r')\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=target_fs, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # === Window and Label ===\n",
    "    #print(f\"Start Windowing: {eeg_id}\", end ='\\r')\n",
    "    windows, labels, timestamps = window_eeg_data(\n",
    "        processed_signal,\n",
    "        resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "\n",
    "    file_start = time.time()\n",
    "    for idx, (window, lbl, ts) in enumerate(zip(windows, labels, timestamps)):\n",
    "\n",
    "        # Optional: Normalize each channel in the window (z-score)\n",
    "        window = (window - window.mean(axis=1, keepdims=True)) / (window.std(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "        save_path = os.path.join(save_folder, f\"{eeg_id}_win{idx}_lbl{lbl}.pt\")\n",
    "        torch.save((window, lbl, eeg_id, ts), save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] ‚úÖ {eeg_id} with {len(windows)} windows in {time.time() - file_start:.2f}s\", end='\\r')\n",
    "\n",
    "# === Overall Time ===\n",
    "overall_time = time.time() - overall_start\n",
    "print(f\"\\n‚è±Ô∏è Total time: {overall_time:.2f} seconds\")\n",
    "print(f\"‚úÖ Finished preprocessing to: {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8c44d-2e44-4590-8b51-62343c04a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "window_size = 4  # seconds\n",
    "step_size = 2    # seconds\n",
    "target_fs = 256  # Hz\n",
    "save_folder = f\"raw_dataset/raw_sequences/win{window_size}_step{step_size}\"\n",
    "print(\"üìÅ Creating folder...\")\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# === Load EEG Dataset ===\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "# === Counters ===\n",
    "count_seizure = 0\n",
    "count_no_seizure = 0\n",
    "\n",
    "# === Timing ===\n",
    "overall_start = time.time()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    # === Preprocessing ===\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=target_fs, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    # === Windowing + Labeling ===\n",
    "    windows, labels, timestamps, used_whole_recording = window_eeg_data(\n",
    "        processed_signal,\n",
    "        resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "    if used_whole_recording:\n",
    "        print(f\"‚ö†Ô∏è Used whole recording as one window for patient {eeg_id}\")\n",
    "\n",
    "    # === Normalize Each Window ===\n",
    "    norm_windows = []\n",
    "    for window in windows:\n",
    "        norm_window = (window - window.mean(axis=1, keepdims=True)) / (window.std(axis=1, keepdims=True) + 1e-8)\n",
    "        norm_windows.append(torch.tensor(norm_window, dtype=torch.float32))\n",
    "\n",
    "    # === Sequence-Level Label ===\n",
    "    sequence_label = seizure_label\n",
    "    if sequence_label == 1:\n",
    "        count_seizure += 1\n",
    "    else:\n",
    "        count_no_seizure += 1\n",
    "        \n",
    "\n",
    "    # === Save All Info Per Patient ===\n",
    "    save_path = os.path.join(save_folder, f\"{eeg_id}_seq.pt\")\n",
    "    torch.save({\n",
    "        \"windows\": norm_windows, # List of tensors [channels, samples]\n",
    "        \"window_labels\": labels,\n",
    "        \"label\": sequence_label,                 # 0 or 1\n",
    "        \"eeg_id\": eeg_id,\n",
    "        \"timestamps\": timestamps,                # One per window\n",
    "        \"seizure_onset\": seizure_onset,          # In seconds\n",
    "        \"seizure_offset\": seizure_offset         # In seconds\n",
    "    }, save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] ‚úÖ {eeg_id} | {len(windows)} windows saved\", end='\\r')\n",
    "\n",
    "# === Overall Timing ===\n",
    "overall_time = time.time() - overall_start\n",
    "print(f\"\\n Total time: {overall_time:.2f} seconds\")\n",
    "print(f\"Final counts -> Seizure sequences: {count_seizure}, Non-seizure sequences: {count_no_seizure}\")\n",
    "print(f\"Finished saving sequences to: {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba45fdc-77b9-4d51-ab15-0a9bc0c0ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "dataset_folder = \"raw_dataset/raw_sequences/win4_step1\"\n",
    "labels = []\n",
    "\n",
    "for fname in os.listdir(dataset_folder):\n",
    "    if fname.endswith(\".pt\"):\n",
    "        path = os.path.join(dataset_folder, fname)\n",
    "        data = torch.load(path, map_location='cpu')\n",
    "        labels.append(data[\"label\"])\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "print(\"Label distribution:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c9338-a1c3-46aa-b52d-90fab1e8ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "from collections import Counter\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "seizure_labels = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "    # seizure_label should be 0 or 1 depending on presence of seizure in recording\n",
    "    seizure_labels.append(seizure_label)\n",
    "\n",
    "label_counts = Counter(seizure_labels)\n",
    "print(\"Raw data label distribution:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df738a2e-10d9-40ff-99c2-02fd0a8e9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from wettbewerb import EEGDataset\n",
    "from new_preprocess import preprocess_signal_with_montages\n",
    "from new_features import window_eeg_data\n",
    "from faster_features import compute_spectrogram\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "window_size = 4  # seconds\n",
    "step_size = 1    # seconds\n",
    "target_fs = 256  # Hz\n",
    "save_folder = f\"raw_dataset/sequences_spectrograms/win{window_size}_step{step_size}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Spectrogram params\n",
    "n_fft = 64\n",
    "hop_length = 32\n",
    "\n",
    "def compute_spectrogram(window, sample_rate=target_fs, n_fft=n_fft, hop_length=hop_length):\n",
    "    window_tensor = torch.tensor(window, dtype=torch.float32)  # shape: [channels, samples]\n",
    "    specs = []\n",
    "    spec_transform = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
    "    for ch in window_tensor:\n",
    "        spec = spec_transform(ch.unsqueeze(0))  # shape [1, freq_bins, time_bins]\n",
    "        specs.append(spec.squeeze(0))           # shape [freq_bins, time_bins]\n",
    "    return torch.stack(specs)                    # shape [channels, freq_bins, time_bins]\n",
    "\n",
    "# Load EEG dataset\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, label = dataset[i]\n",
    "    seizure_label, seizure_onset, seizure_offset = label\n",
    "\n",
    "    montage_names, processed_signal, montage_missing, new_fs = preprocess_signal_with_montages(\n",
    "        channels, raw_data, target_fs=target_fs, original_fs=fs, ids=eeg_id\n",
    "    )\n",
    "    if montage_missing:\n",
    "        print(f\"Skipping {eeg_id} (montage missing)\")\n",
    "        continue\n",
    "\n",
    "    windows, labels, timestamps, used_whole_recording = window_eeg_data(\n",
    "        processed_signal,\n",
    "        resampled_fs=new_fs,\n",
    "        seizure_onset=seizure_onset,\n",
    "        seizure_offset=seizure_offset,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size\n",
    "    )\n",
    "    if used_whole_recording:\n",
    "        print(f\"‚ö†Ô∏è Used whole recording as one window for patient {eeg_id}\")\n",
    "\n",
    "    spectrogram_windows = []\n",
    "    for window in windows:\n",
    "        spec = compute_spectrogram(window)\n",
    "        # Optional: normalize each channel frequency-wise (mean/std)\n",
    "        spec_norm = (spec - spec.mean(dim=(1,2), keepdim=True)) / (spec.std(dim=(1,2), keepdim=True) + 1e-8)\n",
    "        spectrogram_windows.append(spec_norm)\n",
    "\n",
    "    # Sequence label = seizure present in recording (can also use seizure_label directly)\n",
    "    sequence_label = seizure_label\n",
    "\n",
    "    save_path = os.path.join(save_folder, f\"{eeg_id}_seq.pt\")\n",
    "    torch.save({\n",
    "        \"windows\": spectrogram_windows,        # List of tensors [channels, freq_bins, time_bins]\n",
    "        \"label\": sequence_label,\n",
    "        \"eeg_id\": eeg_id,\n",
    "        \"timestamps\": timestamps,\n",
    "        \"seizure_onset\": seizure_onset,\n",
    "        \"seizure_offset\": seizure_offset\n",
    "    }, save_path)\n",
    "\n",
    "    print(f\"[{i+1}/{len(dataset)}] Saved {eeg_id} with {len(windows)} spectrogram windows.\", end='\\r')\n",
    "\n",
    "print(f\"\\nDone. Time elapsed: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6d01d-447b-4da2-a704-e7698e5df1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data_folder = \"../shared_data/training\"\n",
    "patient_id = \"aaaaabnn_s002_t000\"\n",
    "mat_file = os.path.join(data_folder, f\"{patient_id}.mat\")\n",
    "\n",
    "if os.path.exists(mat_file):\n",
    "    mat_data = loadmat(mat_file)\n",
    "    print(f\"Keys in .mat file: {mat_data.keys()}\")\n",
    "\n",
    "    # Replace 'data' with the actual variable name of your EEG signal\n",
    "    eeg_signal = mat_data.get('data')  # e.g. shape (channels, samples)\n",
    "    fs = mat_data.get('fs')  # sampling frequency variable name might differ\n",
    "    \n",
    "    if eeg_signal is None:\n",
    "        print(\"EEG signal variable not found. Check keys above.\")\n",
    "    else:\n",
    "        n_samples = eeg_signal.shape[-1]  # usually last dim is samples\n",
    "        print(f\"Number of samples: {n_samples}\")\n",
    "\n",
    "        if fs is None:\n",
    "            print(\"Sampling frequency 'fs' not found in .mat file, please check metadata or dataset docs.\")\n",
    "        else:\n",
    "            fs_value = fs if isinstance(fs, (int, float)) else fs[0][0]\n",
    "            duration_seconds = n_samples / fs_value\n",
    "            print(f\"Duration of recording: {duration_seconds:.2f} seconds with Frequency of {fs_value}\")\n",
    "else:\n",
    "    print(f\"No .mat file found for patient {patient_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9f1fe-83e7-4af3-afc0-51d9304a022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Original CNN_LSTM\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_channels, input_length, hidden_dim, num_classes=2):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, num_channels, input_length)\n",
    "        cnn_out = self.cnn(dummy_input)\n",
    "        cnn_output_dim = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_output_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, C, S]\n",
    "        \"\"\"\n",
    "        import torch.cuda\n",
    "\n",
    "        B, T, C, S = x.shape\n",
    "        x = x.view(B * T, C, S)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.classifier(lstm_out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798c8401-0e24-498b-84bc-a7addcac1237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 49482\n",
      "Average: 524.33, Min: 1, Median: 297\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "dataset_folder = \"raw_dataset/sequences_spectrograms/win4_step1\"  # or your spectrogram folder\n",
    "file_list = [f for f in os.listdir(dataset_folder) if f.endswith(\".pt\")]\n",
    "\n",
    "max_len = 0\n",
    "lengths = []\n",
    "\n",
    "for i, fname in enumerate(file_list):\n",
    "    path = os.path.join(dataset_folder, fname)\n",
    "    data = torch.load(path, map_location='cpu')\n",
    "    \n",
    "    num_windows = len(data[\"windows\"])\n",
    "    lengths.append(num_windows)\n",
    "    if num_windows > max_len:\n",
    "        max_len = num_windows\n",
    "\n",
    "print(f\"Max sequence length: {max_len} \")\n",
    "print(f\"Average: {sum(lengths)/len(lengths):.2f}, Min: {min(lengths)}, Median: {sorted(lengths)[len(lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5a436e-70ed-4320-9d18-7e81a4706d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/6213\n",
      " Longest recording: aaaaatki_s005_t003\n",
      "Duration: 49485.00 seconds\n",
      "Samples: 12668160\n"
     ]
    }
   ],
   "source": [
    "from wettbewerb import EEGDataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = EEGDataset(\"../shared_data/training\")\n",
    "\n",
    "max_length = 0\n",
    "longest_id = None\n",
    "longest_duration = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    eeg_id, channels, raw_data, fs, _, _ = dataset[i]\n",
    "    duration_sec = raw_data.shape[1] / fs  # samples / Hz\n",
    "    if raw_data.shape[1] > max_length:\n",
    "        max_length = raw_data.shape[1]\n",
    "        longest_id = eeg_id\n",
    "        longest_duration = duration_sec\n",
    "        print(f\"{i}/{len(dataset)}\", end = '\\r')\n",
    "\n",
    "print(f\"\\n Longest recording: {longest_id}\")\n",
    "print(f\"Duration: {longest_duration:.2f} seconds\")\n",
    "print(f\"Samples: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32da6282-8761-402a-bfc9-ce022d76e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\t Dateien wurden geladen.\n",
      "‚ùå ID 'aaaaatki_s005_t003' not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from wettbewerb import load_references\n",
    "\n",
    "# === Config ===\n",
    "training_folder = \"../shared_data/training\"\n",
    "target_id = \"aaaaatki_s005_t003\"  # <- Longest EEG file\n",
    "\n",
    "# === Load metadata ===\n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(training_folder)\n",
    "\n",
    "# === Find info for target EEG ===\n",
    "if target_id in ids:\n",
    "    index = ids.index(target_id)\n",
    "    seizure_label, seizure_onset, seizure_offset = eeg_labels[index]\n",
    "\n",
    "    print(f\"üß† EEG ID: {target_id}\")\n",
    "    print(f\"üìä Seizure label: {seizure_label}\")\n",
    "    print(f\"‚è±Ô∏è Seizure onset: {seizure_onset} sec\")\n",
    "    print(f\"‚è±Ô∏è Seizure offset: {seizure_offset} sec\")\n",
    "else:\n",
    "    print(f\"‚ùå ID '{target_id}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98b0cd-f177-43d5-93e6-05c41c9b6de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
