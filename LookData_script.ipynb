{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b60d2c-df95-47c4-9a97-9400ff2e2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Notebook um sich die Daten genauer anzuschauen'\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import mne\n",
    "from scipy import signal as sig\n",
    "import preprocess\n",
    "import importlib\n",
    "import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58986a89-758f-4664-9d87-6cd454988532",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"../shared_data/training_mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33513c34-51f4-4cf7-bc41-382f76b0dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f13b6f-45f6-4dd5-b8ba-4a944b3a7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_channels = channels[0]  # Use the first sample as reference\n",
    "all_same = True  # Flag to track if all are the same\n",
    "\n",
    "for i, ch in enumerate(channels):\n",
    "    if ch != reference_channels:\n",
    "        all_same = False\n",
    "        print(f\"Sample ID {ids[i]} has different channels.\")\n",
    "        extra = set(ch) - set(reference_channels)\n",
    "        missing = set(reference_channels) - set(ch)\n",
    "        if extra:\n",
    "            print(f\"  Extra channels: {extra}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing channels: {missing}\")\n",
    "\n",
    "if all_same:\n",
    "    print(\"Every sample has the same channels.\")\n",
    "    print(reference_channels)\n",
    "    print(len(reference_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bd596-5727-4665-b11e-b822c7bf1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CNN_dataset\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import os\n",
    "\n",
    "train_folder = \"../shared_data/training\"\n",
    "files = [f for f in os.listdir(train_folder) if f.endswith('.mat')]\n",
    "n_files = len(files)\n",
    "print(f\"found {n_files} files\")\n",
    "\n",
    "index = 0\n",
    "for i in range(0, n_files, 100):\n",
    "    ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder, i)\n",
    "    CNN_dataset.create_cnn_dataset_map(ids, channels, data, sampling_frequencies, reference_systems, eeg_labels, i)\n",
    "    print(f\"created dataset {index}\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f424cf5-76ce-4664-ae1b-87071235333d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75e68209-9a93-4681-b94c-4ba19b1820ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from wettbewerb import get_3montages\n",
    "\n",
    "# Pakete aus dem Vorlesungsbeispiel\n",
    "import mne\n",
    "from scipy import signal as sig\n",
    "import ruptures as rpt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from CNN_model import CNN_EEG\n",
    "from preprocess import process_without_mne\n",
    "from features import feature_extraction\n",
    "from CNN_dataset import window_data_evaluate, create_fixed_grid_maps\n",
    "from glob import glob\n",
    "\n",
    "###Signatur der Methode (Parameter und Anzahl return-Werte) darf nicht verändert werden\n",
    "def predict_labels(channels : List[str], data : np.ndarray, fs : float, reference_system: str, model_name : str='model.json') -> Dict[str,Any]:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : List[str]\n",
    "        Namen der übergebenen Kanäle\n",
    "    data : ndarray\n",
    "        EEG-Signale der angegebenen Kanäle\n",
    "    fs : float\n",
    "        Sampling-Frequenz der Signale.\n",
    "    reference_system :  str\n",
    "        Welches Referenzsystem wurde benutzt, \"Bezugselektrode\", nicht garantiert korrekt!\n",
    "    model_name : str\n",
    "        Name eures Models,das ihr beispielsweise bei Abgabe genannt habt. \n",
    "        Kann verwendet werden um korrektes Model aus Ordner zu laden\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : Dict[str,Any]\n",
    "        enthält Vorhersage, ob Anfall vorhanden und wenn ja wo (Onset+Offset)\n",
    "    '''\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Euer Code ab hier  \n",
    "\n",
    "    # Initialisiere Return (Ergebnisse)\n",
    "    seizure_present = True # gibt an ob ein Anfall vorliegt\n",
    "    seizure_confidence = 0.5 # gibt die Unsicherheit des Modells an (optional)\n",
    "    onset = 4.2   # gibt den Beginn des Anfalls an (in Sekunden)\n",
    "    onset_confidence = 0.99 # gibt die Unsicherheit bezüglich des Beginns an (optional)\n",
    "    offset = 999999  # gibt das Ende des Anfalls an (optional)\n",
    "    offset_confidence = 0   # gibt die Unsicherheit bezüglich des Endes an (optional)\n",
    "\n",
    "    # Modell Aufsetzen\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device(\"cpu\")\n",
    "    '''\n",
    "    model = torch.load(model_name, map_location=device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    '''\n",
    "    #Daten vorbereiten\n",
    "    window_size = 4.0\n",
    "    step_size = 1\n",
    "    standard_channels = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "    n_nodes = len(standard_channels)\n",
    "\n",
    "    processed_input = process_without_mne(data, fs, channels, reference_system, fs)\n",
    "    \n",
    "    channel_map = {ch: idx for idx, ch in enumerate(channels)}\n",
    "    pad_data = np.zeros((n_nodes, processed_input.shape[1]))\n",
    "    for j, ch in enumerate(standard_channels):\n",
    "        if ch in channel_map:\n",
    "            pad_data[j] = processed_input[channel_map[ch]]\n",
    "    \n",
    "    windows = window_data_evaluate(pad_data, fs, window_size, step_size)\n",
    "    data_for_class = []\n",
    "    # Feature extraction and brain map calculation\n",
    "    for win in windows:\n",
    "        features = feature_extraction(win, fs) # shape: (n_channels, n_features)\n",
    "        assert not np.isnan(features).any(), \"NaN in features!\"\n",
    "        brain_map = create_fixed_grid_maps(features, channels)\n",
    "        assert not np.isnan(brain_map).any(), \"NaN in brain_map!\"\n",
    "        x = torch.tensor(brain_map, dtype = torch.float)\n",
    "        data_for_class.append(x)\n",
    "        \n",
    "\n",
    "    # Klassifikation\n",
    "    predictions_per_window =[]\n",
    "    with torch.no_grad():\n",
    "        for feature_map in data_for_class:\n",
    "            feature_map = feature_map.unsqueeze(0).to(device)\n",
    "            #output = model(feature_map)\n",
    "            #predicted_class = torch.argmax(output, dim=1).item()\n",
    "            predicted_class = predictions_ensemble(feature_map,model_name,device)\n",
    "            predictions_per_window.append(predicted_class)\n",
    "    \n",
    "    seizure_present = False\n",
    "    '''\n",
    "    if 1 in predictions_per_window:\n",
    "        seizure_present = True\n",
    "        first_idx = predictions_per_window.index(1)\n",
    "        time_first = first_idx * step_size\n",
    "        onset = time_first\n",
    "    '''\n",
    "    for i in range(len(predictions_per_window) - 1):\n",
    "        if predictions_per_window[i] == 1 and predictions_per_window[i + 1] == 1:\n",
    "            seizure_present = True\n",
    "            time_first = i * step_size\n",
    "            onset = time_first\n",
    "            break\n",
    "    '''\n",
    "    # Hier könnt ihr euer vortrainiertes Modell laden (Kann auch aus verschiedenen Dateien bestehen)\n",
    "    model = MyCNN()\n",
    "    model.load_state_dict(torch.load(model_name, map_location='cpu'))\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    # Wende Beispielcode aus Vorlesung an \n",
    "    \n",
    "    _montage, _montage_data, _is_missing = get_3montages(channels, data)\n",
    "    signal_std = np.zeros(len(_montage))\n",
    "    for j, signal_name in enumerate(_montage):\n",
    "        # Ziehe erste Montage des EEG\n",
    "        signal = _montage_data[j]\n",
    "        # Wende Notch-Filter an um Netzfrequenz zu dämpfen\n",
    "        signal_notch = mne.filter.notch_filter(x=signal, Fs=fs, freqs=np.array([50.,100.]), n_jobs=2, verbose=False)\n",
    "        # Wende Bandpassfilter zwischen 0.5Hz und 70Hz um Rauschen aus dem Signal zu filtern\n",
    "        signal_filter = mne.filter.filter_data(data=signal_notch, sfreq=fs, l_freq=0.5, h_freq=70.0, n_jobs=2, verbose=False)\n",
    "        \n",
    "        # Berechne short time fourier transformation des Signal: signal_filtered = filtered signal of channel, fs = sampling frequency, nperseg = length of each segment\n",
    "        # Output f= array of sample frequencies, t = array of segment times, Zxx = STFT of signal\n",
    "        f, t, Zxx = sig.stft(signal_filter, fs, nperseg=fs * 3)\n",
    "        # Berechne Schrittweite der Frequenz\n",
    "        df = f[1] - f[0]\n",
    "        # Berechne Engergie (Betrag) basierend auf Real- und Imaginärteil der STFT\n",
    "        E_Zxx = np.sum(Zxx.real ** 2 + Zxx.imag ** 2, axis=0) * df\n",
    "        \n",
    "        signal_std[j] = np.std(signal_filter)\n",
    "\n",
    "\n",
    "\n",
    "        # Erstelle neues Array in der ersten Iteration pro Patient\n",
    "        if j == 0:\n",
    "            # Initilisiere Array mit Energiesignal des ersten Kanals\n",
    "            E_array = np.array(E_Zxx)\n",
    "        else:\n",
    "            # Füge neues Energiesignal zu vorhandenen Kanälen hinzu (stack it)\n",
    "            E_array = np.vstack((E_array, np.array(E_Zxx)))\n",
    "            \n",
    "    # Berechne Feature zur Seizure Detektion\n",
    "    signal_std_max = signal_std.max()\n",
    "    # Klassifiziere Signal\n",
    "    seizure_present = signal_std_max>th_opt\n",
    "    \n",
    "    # Berechne Gesamtenergie aller Kanäle für jeden Zeitppunkt\n",
    "    E_total = np.sum(E_array, axis=0)\n",
    "    # Berechne Stelle der maximalen Energie\n",
    "    max_index = E_total.argmax()\n",
    "\n",
    "    # Berechne \"changepoints\" der Gesamtenergie\n",
    "    # Falls Maximum am Anfang des Signals ist muss der Onset ebenfalls am Anfang sein und wir können keinen \"changepoint\" berechnen\n",
    "    if max_index == 0:\n",
    "        onset = 0.0\n",
    "        onset_confidence = 0.2\n",
    "        \n",
    "    else:\n",
    "        # Berechne \"changepoint\" mit dem ruptures package\n",
    "        # Setup für  \"linearly penalized segmentation method\" zur Detektion von changepoints im Signal mi rbf cost function\n",
    "        algo = rpt.Pelt(model=\"rbf\").fit(E_total)\n",
    "        # Berechne sortierte Liste der changepoints, pen = penalty value\n",
    "        result = algo.predict(pen=10)\n",
    "        #Indices sind ums 1 geshiftet\n",
    "        result1 = np.asarray(result) - 1\n",
    "        # Selektiere changepoints vor Maximum\n",
    "        result_red = result1[result1 < max_index]\n",
    "        # Falls es mindestens einen changepoint gibt nehmen wir den nächsten zum Maximum\n",
    "        if len(result_red)<1:\n",
    "            # Falls keine changepoint gefunden wurde raten wir, dass er \"nahe\" am Maximum ist\n",
    "            print('No changepoint, taking maximum')\n",
    "            onset_index = max_index\n",
    "        else:\n",
    "            # Der changepoint entspricht gerade dem Onset \n",
    "            onset_index = result_red[-1]\n",
    "        # Gebe Onset zurück\n",
    "        onset = t[onset_index]      \n",
    "     \n",
    "     \n",
    "    '''\n",
    "#------------------------------------------------------------------------------  \n",
    "    prediction = {\"seizure_present\":seizure_present,\"seizure_confidence\":seizure_confidence,\n",
    "                   \"onset\":onset,\"onset_confidence\":onset_confidence,\"offset\":offset,\n",
    "                   \"offset_confidence\":offset_confidence}\n",
    "  \n",
    "    return prediction # Dictionary mit prediction - Muss unverändert bleiben!\n",
    "                               \n",
    "                               \n",
    "        \n",
    "def predictions_ensemble(feature,model_name,device):\n",
    "    \n",
    "    file_paths = sorted([os.path.join(model_name, f) for f in os.listdir(model_name) if f.endswith(\".pth\")])\n",
    "\n",
    "    probas = torch.zeros(2).to(device)  # 2 Klassen\n",
    "    with torch.no_grad():\n",
    "        for path in file_paths:\n",
    "            model = torch.load(path, map_location=device)\n",
    "            model.eval()\n",
    "            output = model(feature)  # shape: [1, 2]\n",
    "            probas += torch.softmax(output.squeeze(), dim=0)  # → shape: [2]\n",
    "\n",
    "    prediction = probas / len(file_paths)  # shape: [2]\n",
    "    y_pred = (prediction[1] > 0.5).long()  # ← sicher!\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be24367d-a342-4c17-a1d5-eccd50a5550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t Dateien wurden geladen.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_INITIALIZED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ids, channels, data, sampling_frequencies, reference_systems, eeg_labels \u001b[38;5;241m=\u001b[39m load_references(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../shared_data/training_mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m99\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampling_frequencies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreference_systems\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels_strat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 92\u001b[0m, in \u001b[0;36mpredict_labels\u001b[0;34m(channels, data, fs, reference_system, model_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m         feature_map \u001b[38;5;241m=\u001b[39m feature_map\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m#output = model(feature_map)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m#predicted_class = torch.argmax(output, dim=1).item()\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m         predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         predictions_per_window\u001b[38;5;241m.\u001b[39mappend(predicted_class)\n\u001b[1;32m     95\u001b[0m seizure_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 205\u001b[0m, in \u001b[0;36mpredictions_ensemble\u001b[0;34m(feature, model_name, device)\u001b[0m\n\u001b[1;32m    203\u001b[0m         model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    204\u001b[0m         model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 205\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [1, 2]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         probas \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output\u001b[38;5;241m.\u001b[39msqueeze(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# → shape: [2]\u001b[39;00m\n\u001b[1;32m    208\u001b[0m prediction \u001b[38;5;241m=\u001b[39m probas \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_paths)  \u001b[38;5;66;03m# shape: [2]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/wki-sose25/CNN_model.py:29\u001b[0m, in \u001b[0;36mCNN_EEG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"
     ]
    }
   ],
   "source": [
    "import CNN_dataset\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import os\n",
    "\n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(\"../shared_data/training_mini\", 99)\n",
    "print(predict_labels(channels[0],data[0],sampling_frequencies[0],reference_systems[0],\"models_strat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbb7cf-78d8-4b95-b127-ff2357b6c4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
