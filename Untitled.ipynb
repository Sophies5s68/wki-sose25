{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8a74f7-6c08-4d1f-84f3-cee576344045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6213 files\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 0\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 1\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 2\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 3\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 4\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 5\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 6\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 7\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 8\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 9\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 10\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 11\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 12\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 13\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 14\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 15\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 16\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 17\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 18\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 19\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 20\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 21\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 22\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 23\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 24\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 25\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 26\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 27\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 28\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 29\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 30\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 31\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 32\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 33\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 34\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 35\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 36\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 37\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 38\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 39\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 40\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 41\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 42\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 43\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 44\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 45\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 46\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 47\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 48\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 49\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 50\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 51\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 52\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 53\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 54\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 55\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 56\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 57\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 58\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 59\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 60\n",
      "100\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 61\n",
      "13\t Dateien wurden geladen.\n",
      "Dataset mit Maps erstellt\n",
      "created dataset 62\n"
     ]
    }
   ],
   "source": [
    "import CNN_dataset\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import os\n",
    "\n",
    "train_folder = \"../shared_data/training\"\n",
    "output_folder = \"data_test\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "files = [f for f in os.listdir(train_folder) if f.endswith('.mat')]\n",
    "n_files = len(files)\n",
    "print(f\"found {n_files} files\")\n",
    "\n",
    "index = 0\n",
    "for i in range(0, n_files, 100):\n",
    "    ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder, i)\n",
    "    CNN_dataset.create_cnn_dataset_map(ids, channels, data, sampling_frequencies, reference_systems, eeg_labels,output_folder, i)\n",
    "    print(f\"created dataset {index}\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f18538-8e6f-45fd-a458-fcdc4ce44850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "x NaN: tensor(False)\n",
      "x Inf: tensor(False)\n",
      "y NaN: tensor(False)\n",
      "y Inf: tensor(False)\n",
      "x stats - min: -3.226975917816162 max: 4.242640495300293 mean: -7.947286162490741e-10 std: 0.8718403577804565\n",
      "starting training on cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 11, 3, 3], expected input[16, 18, 5, 5] to have 11 channels, but got 18 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 136\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#Training \u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 136\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     test_acc, y_true, y_pred \u001b[38;5;241m=\u001b[39m CNN_model\u001b[38;5;241m.\u001b[39mevaluate_model(model, test_loader,device)\n\u001b[1;32m    139\u001b[0m     fold_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/wki-sose25/CNN_model.py:65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 65\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(out)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(out)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN oder Inf im Output des Modells!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/wki-sose25/CNN_model.py:45\u001b[0m, in \u001b[0;36mCNN_EEG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x))\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 11, 3, 3], expected input[16, 18, 5, 5] to have 11 channels, but got 18 channels instead"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset, Subset,TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "# Datenordner einladen:\n",
    "data_folder = \"data_test\"\n",
    "file_paths = sorted(glob(os.path.join(data_folder, \"*.pt\")))\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\"Unterordner nicht gefunden\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Modell instantiieren\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "metrics = []\n",
    "batch_nr = 0\n",
    "train_dataset_global = []\n",
    "test_dataset_global =[]\n",
    "\n",
    "all_x = []\n",
    "all_y = []\n",
    "all_id = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    dataset = torch.load(file_path)\n",
    "    for x, y, gruppe in dataset:\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        all_x.append(x)\n",
    "        all_y.append(int(y))\n",
    "        all_id.append(gruppe)\n",
    "        batch_nr = batch_nr + 1\n",
    "\n",
    "# In NumPy konvertieren\n",
    "all_x_np = np.stack([x.numpy() for x in all_x])\n",
    "all_y_np = np.array(all_y)\n",
    "all_id_np = np.array(all_id)\n",
    "    \n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'x': list(all_x_np),  # wichtig: Liste von Arrays\n",
    "    'y': all_y_np,\n",
    "    'id': all_id_np\n",
    "})\n",
    "# stratified == erhält Klassengewichtung für alle Folds und Groupkfold = keine Überschneidung Patienten\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "num_epochs = 30\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(df['x'], df['y'], df['id'])):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    '''\n",
    "    # Balancieren der Testdaten\n",
    "    train_pos = train_df[train_df['y'] == 1]\n",
    "    train_neg = train_df[train_df['y'] == 0].sample(len(train_pos), random_state=42)\n",
    "    train_bal = pd.concat([train_pos, train_neg]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    X_train = np.stack(train_bal['x'].values)\n",
    "    y_train = train_bal['y'].values\n",
    "    \n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    '''\n",
    "    X_train = np.stack(train_df['x'].values)\n",
    "    y_train = train_df['y'].values\n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    \n",
    "    # Berechnung der Klassengewichte\n",
    "    classes = np.unique(all_y_np)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    #Modell instantiieren\n",
    "    import CNN_model\n",
    "    importlib.reload(CNN_model)\n",
    "    model = CNN_model.CNN_EEG(in_channels=11, n_classes=2)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 1e-4)\n",
    "    num_epochs = 30\n",
    "\n",
    "\n",
    "    # Wenn X_train und y_train numpy arrays sind:\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Gleiches für Testdaten:\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        print(\"x NaN:\", torch.isnan(x).any())\n",
    "        print(\"x Inf:\", torch.isinf(x).any())\n",
    "        print(\"y NaN:\", torch.isnan(y).any())\n",
    "        print(\"y Inf:\", torch.isinf(y).any())\n",
    "        print(\"x stats - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item(), \"std:\", x.std().item())\n",
    "        break\n",
    "\n",
    "    print(f\"starting training on {device}\")\n",
    "    \n",
    "    # Metrics tracking\n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "\n",
    "    #Training \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn,device)\n",
    "        test_acc, y_true, y_pred = CNN_model.evaluate_model(model, test_loader,device)\n",
    "        \n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_train_accuracies.append(train_acc)\n",
    "        fold_test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Save metrics for this fold\n",
    "    train_losses.append(fold_train_losses)\n",
    "    train_accuracies.append(fold_train_accuracies)\n",
    "    test_accuracies.append(fold_test_accuracies)\n",
    "     \n",
    "    # Confusion Matrix of one fold\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics.append((test_acc,train_acc,y_pred,cm))\n",
    "\n",
    "    print(f\"Metrics last epoch,fold: {fold} test_acc: {test_acc}, train_acc: {train_acc}\")\n",
    "    \n",
    "    \n",
    "    data = data_folder.split(\"/\")[1]\n",
    "    path = \"models_strat/\"\n",
    "    save_path = path + data #Hier ändern für Ordner\n",
    "    os.makedirs(save_path, exist_ok=True)  # Verzeichnis erstellen, falls es noch nicht existiert\n",
    "\n",
    "    torch.save(model, os.path.join(save_path, f\"model_{fold}.pth\"))\n",
    "\n",
    "print(\"finished training\")\n",
    "\n",
    "#Print final metrics and confusion matrix\n",
    "for fold, (test_acc, train_acc, y_pred, cm) in enumerate(metrics):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(f\"  Test accuracy:  {test_acc:.2f}\")\n",
    "    print(f\"  Train accuracy: {train_acc:.2f}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Kein Anfall\", \"Anfall\"])\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(\"Confusion Matrix (Test Set)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics per fold\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "for fold in range(len(train_losses)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses[fold], label='Train Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accuracies[fold], label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies[fold], label='Test Accuracy')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Accuracy - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot average across folds\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_train_acc = np.mean(train_accuracies, axis=0)\n",
    "mean_test_acc = np.mean(test_accuracies, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, mean_train_loss, label='Avg Train Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Average Training Loss Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, mean_train_acc, label='Avg Train Accuracy')\n",
    "plt.plot(epochs, mean_test_acc, label='Avg Test Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Average Training Accuracy Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20c905-12a7-41e2-98d3-ec468628371e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
