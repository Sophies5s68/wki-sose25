{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a74f7-6c08-4d1f-84f3-cee576344045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CNN_dataset\n",
    "from wettbewerb import load_references, get_3montages\n",
    "import os\n",
    "\n",
    "train_folder = \"../shared_data/training\"\n",
    "output_folder = \"data_test\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "files = [f for f in os.listdir(train_folder) if f.endswith('.mat')]\n",
    "n_files = len(files)\n",
    "print(f\"found {n_files} files\")\n",
    "\n",
    "index = 0\n",
    "for i in range(0, n_files, 100):\n",
    "    ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(train_folder, i)\n",
    "    CNN_dataset.create_cnn_dataset_map(ids, channels, data, sampling_frequencies, reference_systems, eeg_labels,output_folder, i)\n",
    "    print(f\"created dataset {index}\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f18538-8e6f-45fd-a458-fcdc4ce44850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset, Subset,TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "# Datenordner einladen:\n",
    "data_folder = \"data_test\"\n",
    "file_paths = sorted(glob(os.path.join(data_folder, \"*.pt\")))\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\"Unterordner nicht gefunden\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Modell instantiieren\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "metrics = []\n",
    "batch_nr = 0\n",
    "train_dataset_global = []\n",
    "test_dataset_global =[]\n",
    "\n",
    "all_x = []\n",
    "all_y = []\n",
    "all_id = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    dataset = torch.load(file_path)\n",
    "    for x, y, gruppe in dataset:\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        all_x.append(x)\n",
    "        all_y.append(int(y))\n",
    "        all_id.append(gruppe)\n",
    "        batch_nr = batch_nr + 1\n",
    "\n",
    "# In NumPy konvertieren\n",
    "all_x_np = np.stack([x.numpy() for x in all_x])\n",
    "all_y_np = np.array(all_y)\n",
    "all_id_np = np.array(all_id)\n",
    "    \n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'x': list(all_x_np),  # wichtig: Liste von Arrays\n",
    "    'y': all_y_np,\n",
    "    'id': all_id_np\n",
    "})\n",
    "# stratified == erhält Klassengewichtung für alle Folds und Groupkfold = keine Überschneidung Patienten\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "num_epochs = 30\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(df['x'], df['y'], df['id'])):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    '''\n",
    "    # Balancieren der Testdaten\n",
    "    train_pos = train_df[train_df['y'] == 1]\n",
    "    train_neg = train_df[train_df['y'] == 0].sample(len(train_pos), random_state=42)\n",
    "    train_bal = pd.concat([train_pos, train_neg]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    X_train = np.stack(train_bal['x'].values)\n",
    "    y_train = train_bal['y'].values\n",
    "    \n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    '''\n",
    "    X_train = np.stack(train_df['x'].values)\n",
    "    y_train = train_df['y'].values\n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    \n",
    "    # Berechnung der Klassengewichte\n",
    "    classes = np.unique(all_y_np)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    #Modell instantiieren\n",
    "    import CNN_model\n",
    "    importlib.reload(CNN_model)\n",
    "    model = CNN_model.CNN_EEG(in_channels=11, n_classes=2)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 1e-4)\n",
    "    num_epochs = 30\n",
    "\n",
    "\n",
    "    # Wenn X_train und y_train numpy arrays sind:\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Gleiches für Testdaten:\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        print(\"x NaN:\", torch.isnan(x).any())\n",
    "        print(\"x Inf:\", torch.isinf(x).any())\n",
    "        print(\"y NaN:\", torch.isnan(y).any())\n",
    "        print(\"y Inf:\", torch.isinf(y).any())\n",
    "        print(\"x stats - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item(), \"std:\", x.std().item())\n",
    "        break\n",
    "\n",
    "    print(f\"starting training on {device}\")\n",
    "    \n",
    "    # Metrics tracking\n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "\n",
    "    #Training \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn,device)\n",
    "        test_acc, y_true, y_pred = CNN_model.evaluate_model(model, test_loader,device)\n",
    "        \n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_train_accuracies.append(train_acc)\n",
    "        fold_test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Save metrics for this fold\n",
    "    train_losses.append(fold_train_losses)\n",
    "    train_accuracies.append(fold_train_accuracies)\n",
    "    test_accuracies.append(fold_test_accuracies)\n",
    "     \n",
    "    # Confusion Matrix of one fold\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics.append((test_acc,train_acc,y_pred,cm))\n",
    "\n",
    "    print(f\"Metrics last epoch,fold: {fold} test_acc: {test_acc}, train_acc: {train_acc}\")\n",
    "    \n",
    "    \n",
    "    data = data_folder.split(\"/\")[1]\n",
    "    path = \"models_strat/\"\n",
    "    save_path = path + data #Hier ändern für Ordner\n",
    "    os.makedirs(save_path, exist_ok=True)  # Verzeichnis erstellen, falls es noch nicht existiert\n",
    "\n",
    "    torch.save(model, os.path.join(save_path, f\"model_{fold}.pth\"))\n",
    "\n",
    "print(\"finished training\")\n",
    "\n",
    "#Print final metrics and confusion matrix\n",
    "for fold, (test_acc, train_acc, y_pred, cm) in enumerate(metrics):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(f\"  Test accuracy:  {test_acc:.2f}\")\n",
    "    print(f\"  Train accuracy: {train_acc:.2f}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Kein Anfall\", \"Anfall\"])\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(\"Confusion Matrix (Test Set)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics per fold\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "for fold in range(len(train_losses)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses[fold], label='Train Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accuracies[fold], label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies[fold], label='Test Accuracy')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Accuracy - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot average across folds\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_train_acc = np.mean(train_accuracies, axis=0)\n",
    "mean_test_acc = np.mean(test_accuracies, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, mean_train_loss, label='Avg Train Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Average Training Loss Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, mean_train_acc, label='Avg Train Accuracy')\n",
    "plt.plot(epochs, mean_test_acc, label='Avg Test Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Average Training Accuracy Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20c905-12a7-41e2-98d3-ec468628371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.signal import resample\n",
    "from wettbewerb import get_6montages\n",
    "\n",
    "def process_sample(row, target_fs=64):\n",
    "    fs = row[\"fs\"]\n",
    "    raw_data = row[\"data\"]      # (T, N)\n",
    "    all_channels = row[\"channels\"]\n",
    "    label = row[\"label\"]  \n",
    "    \n",
    "    #data_np = np.stack(raw_data, axis=1).astype(np.float32)\n",
    "    montages, montage_data, montage_missing = get_6montages(all_channels, raw_data)      # (T, 6)\n",
    "\n",
    "    # Downsampling\n",
    "    T_new = int(len(montage_data[1]) * target_fs / fs)\n",
    "    data = resample(montage_data, T_new, axis=1)\n",
    "    data = data.T\n",
    "    # Zielmaske (Onset = 1 ab Zeitpunkt)\n",
    "    onset_s = label[1]\n",
    "    onset_idx = int(onset_s * target_fs)\n",
    "    y = torch.zeros(T_new, dtype=torch.float32)\n",
    "    y[onset_idx:] = 1.0\n",
    "\n",
    "    x = torch.tensor(data, dtype=torch.float32)  # (T, 6)\n",
    "    return x, y, T_new\n",
    "\n",
    "def split_and_save_tensor_batches(pickle_path, output_dir, batch_size=64, target_fs=64, random_seed=42):\n",
    "    print(\"Lade Pickle-Datei...\")\n",
    "    df = pd.read_pickle(pickle_path)\n",
    "    print(f\"DataFrame Größe: {df.shape}\")\n",
    "\n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    print(\"DataFrame geshufflet.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    num_batches = int(np.ceil(len(df) / batch_size))\n",
    "    print(f\"Speichere {num_batches} Batches mit Batch-Größe {batch_size} ...\")\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_df = df.iloc[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "        x_list, y_list, len_list = [], [], []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            try:\n",
    "                x, y, L = process_sample(row, target_fs)\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "                len_list.append(L)\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler in Zeile {row['ids']}: {e}\")\n",
    "                continue\n",
    "\n",
    "        x_padded = pad_sequence(x_list, batch_first=True)      # (B, T_max, 6)\n",
    "        y_padded = pad_sequence(y_list, batch_first=True)      # (B, T_max)\n",
    "        lengths = torch.tensor(len_list, dtype=torch.long)     # (B,)\n",
    "\n",
    "        batch_tensor = {\n",
    "            \"x\": x_padded,\n",
    "            \"y\": y_padded,\n",
    "            \"lengths\": lengths\n",
    "        }\n",
    "\n",
    "        batch_path = os.path.join(output_dir, f\"batch_{i:03d}.pt\")\n",
    "        torch.save(batch_tensor, batch_path)\n",
    "        print(f\"Batch {i+1}/{num_batches} gespeichert: {batch_path}\")\n",
    "\n",
    "    print(\"Fertig.\")\n",
    "\n",
    "# Beispiel-Nutzung\n",
    "batch_dir  = \"LSTM/positive_filtered_100Hz.pkl\"\n",
    "output_dir = \"LSTM/tensor_batches\"\n",
    "batch_size = 64\n",
    "\n",
    "split_and_save_tensor_batches(batch_dir, output_dir, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd70e9d-8868-4280-83b6-2cf07ce5625d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819ea460-2792-4e4d-9570-9a90e6ce0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-wki_team_3/wki-sose25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/home/jupyter-wki_team_3/wki-sose25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab2675-243a-4d60-8a59-ce2c547cf2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class OnsetLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=64, num_layers=2):\n",
    "        super(OnsetLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)              # (B, T, H*2)\n",
    "        out = self.classifier(out).squeeze(-1)  # (B, T)\n",
    "        return out  # Logits, keine Sigmoid hier!\n",
    "\n",
    "# Dataset-Klasse zum Laden der gespeicherten .pt-Batches\n",
    "class EEGOnsetBatchDataset(Dataset):\n",
    "    def __init__(self, batch_folder):\n",
    "        self.batch_paths = sorted([os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.endswith('.pt')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.load(self.batch_paths[idx])\n",
    "\n",
    "# Training & Evaluation\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        mask = torch.arange(x.shape[1], device=device)[None, :] < batch['lengths'][:, None].to(device)  # (B, T)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)  # (B, T)\n",
    "        loss = criterion(logits[mask], y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        x = batch['x'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        mask = torch.arange(x.shape[1], device=device)[None, :] < batch['lengths'][:, None].to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits[mask], y[mask])\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Hauptfunktion\n",
    "def run_training(train_dir, val_dir, epochs=10, batch_size=1, lr=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    train_dataset = EEGOnsetBatchDataset(train_dir)\n",
    "    val_dataset = EEGOnsetBatchDataset(val_dir)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = 1, shuffle=True,collate_fn=lambda x: x[0])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1,collate_fn=lambda x: x[0])\n",
    "\n",
    "    model = OnsetLSTM().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"onset_lstm.pt\")\n",
    "    print(\"Modell gespeichert.\")\n",
    "\n",
    "# Beispielaufruf\n",
    "run_training(\"LSTM/tensor_batches/train\", \"LSTM/tensor_batches/val\", epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1df665-7ea8-4412-9a15-dc2d7535f25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x', 'y', 'lengths'])\n",
      "torch.Size([64, 107651, 6])\n",
      "torch.Size([64, 107651])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch = torch.load(\"LSTM/tensor_batches/train/batch_010.pt\")\n",
    "print(batch.keys())\n",
    "print(batch['x'].shape)  # z. B. (64, 7331, 6)\n",
    "print(batch['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a451389-9ded-4a1d-ac94-19f87d6c279e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
