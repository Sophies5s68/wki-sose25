{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc859bc-d083-4cc1-9fc9-f8f84c50c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class EEGOnsetLSTM(nn.Module):\n",
    "    def __init__(self, n_channels, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_channels,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        # Bidirektional → hidden_size * 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # Output: 1 Wert pro Sample (Onset)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: [batch, seq_len, n_channels]\n",
    "        # lengths: [batch] echte Längen ohne Padding\n",
    "\n",
    "        # Sortiere nach Länge absteigend (notwendig für pack_padded_sequence)\n",
    "        lengths_sorted, sorted_idx = lengths.sort(descending=True)\n",
    "        x_sorted = x[sorted_idx]\n",
    "\n",
    "        # Packe die Sequenzen\n",
    "        packed_input = pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "\n",
    "        # LSTM vorwärts\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers * num_directions, batch, hidden_size]\n",
    "        # Für bidirektionales LSTM: 2 Richtungen → wir konkateniere die letzten Layerstates\n",
    "\n",
    "        # Wir nehmen die letzten Layerstates beider Richtungen\n",
    "        # Layerindex: -1 (letzte Schicht)\n",
    "        # hn shape: [2, batch, hidden_size]\n",
    "        # Wir transponieren und flatten zu [batch, hidden_size*2]\n",
    "        hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)\n",
    "        hn_last_layer = hn[-1]  # Form: [2, batch, hidden_size]\n",
    "        hn_cat = torch.cat((hn_last_layer[0], hn_last_layer[1]), dim=1)  # [batch, hidden_size*2]\n",
    "\n",
    "        # Rücksortieren, um ursprüngliche Reihenfolge wiederherzustellen\n",
    "        _, original_idx = sorted_idx.sort()\n",
    "        hn_cat = hn_cat[original_idx]\n",
    "\n",
    "        # Fully connected zum Onset (regression)\n",
    "        output = self.fc(hn_cat).squeeze(1)  # [batch]\n",
    "\n",
    "        return output\n",
    "\n",
    "# Beispiel Daten und Labels (Dummy)\n",
    "batch_size = 3\n",
    "n_channels = 21\n",
    "seq_lengths = torch.tensor([1000, 800, 600])  # variable Längen\n",
    "max_len = seq_lengths.max()\n",
    "\n",
    "# Zufällige Daten mit Padding (Nullen)\n",
    "x = torch.zeros(batch_size, max_len, n_channels)\n",
    "for i, length in enumerate(seq_lengths):\n",
    "    x[i, :length] = torch.randn(length, n_channels)\n",
    "\n",
    "# Beispiel-Onsets als Indexwerte (ground truth)\n",
    "labels = torch.tensor([400, 350, 200], dtype=torch.float32)\n",
    "\n",
    "# Modell, Optimizer, Loss\n",
    "model = EEGOnsetLSTM(n_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Trainingsschritt (ein Beispiel)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = model(x, seq_lengths)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Predicted Onsets: {outputs.detach().cpu().numpy()}\")\n",
    "print(f\"True Onsets: {labels.cpu().numpy()}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf4828-c311-4a4c-8402-e54195f1ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Große Pickle datei einlesen und in batches abspeichern, für verbessertes Training\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def split_and_save_batches(pickle_path, output_dir, batch_size=64, random_seed=42):\n",
    "    # 1. Lade kompletten DataFrame\n",
    "    print(\"Lade Pickle-Datei...\")\n",
    "    df = pd.read_pickle(pickle_path)\n",
    "    print(f\"DataFrame Größe: {df.shape}\")\n",
    "\n",
    "    # 2. Shuffle DataFrame\n",
    "    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    print(\"DataFrame geshufflet.\")\n",
    "\n",
    "    # 3. Batchweise speichern\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    num_batches = int(np.ceil(len(df) / batch_size))\n",
    "    print(f\"Speichere {num_batches} Batches mit Batch-Größe {batch_size} ...\")\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_df = df.iloc[i*batch_size : (i+1)*batch_size]\n",
    "\n",
    "        batch_path = os.path.join(output_dir, f\"batch_{i:03d}.pkl\")\n",
    "        batch_df.to_pickle(batch_path)\n",
    "        print(f\"Batch {i+1}/{num_batches} gespeichert: {batch_path}\")\n",
    "\n",
    "    print(\"Fertig.\")\n",
    "\n",
    "# Beispiel Nutzung\n",
    "pickle_path = \"positive_filtered_100Hz.pkl\"\n",
    "output_dir = \"batches_LSTM\"\n",
    "batch_size = 64\n",
    "\n",
    "split_and_save_batches(pickle_path, output_dir, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386e102-d6db-4f3a-a963-d86355d06a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class LazyEEGDataset(Dataset):\n",
    "    def __init__(self, batch_dir, n_channels=21, eeg_col='data', label_col='label', fixed_batch_size=64):\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_files = sorted([os.path.join(batch_dir, f) for f in os.listdir(batch_dir) if f.endswith('.pkl')])\n",
    "        self.eeg_col = eeg_col\n",
    "        self.label_col = label_col\n",
    "        self.fixed_batch_size = fixed_batch_size\n",
    "        \n",
    "        # Länge des Datasets = (Anzahl der Dateien - 1) * fixed_batch_size + Größe der letzten Datei\n",
    "        last_file_df = pd.read_pickle(self.batch_files[-1])\n",
    "        last_file_len = len(last_file_df)\n",
    "        self.total_len = (len(self.batch_files) - 1) * fixed_batch_size + last_file_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Berechne Dateiindex und inneren Index mit fester Batchgröße\n",
    "        file_idx = idx // self.fixed_batch_size\n",
    "        inner_idx = idx % self.fixed_batch_size\n",
    "        \n",
    "        # Falls Index in der letzten Datei liegt, korrigiere inner_idx\n",
    "        if file_idx == len(self.batch_files) - 1:\n",
    "            # Letzte Datei kann kleiner sein als fixed_batch_size\n",
    "            last_file_df = pd.read_pickle(self.batch_files[file_idx])\n",
    "            if inner_idx >= len(last_file_df):\n",
    "                raise IndexError(f\"Index {idx} außerhalb der Range der letzten Datei mit {len(last_file_df)} Samples\")\n",
    "            row = last_file_df.iloc[inner_idx]\n",
    "        else:\n",
    "            df = pd.read_pickle(self.batch_files[file_idx])\n",
    "            row = df.iloc[inner_idx]\n",
    "\n",
    "        seq = row[self.eeg_col]\n",
    "        if isinstance(seq, list):\n",
    "            seq = torch.tensor(seq).float()\n",
    "        elif isinstance(seq, np.ndarray):\n",
    "            seq = torch.from_numpy(seq.T).float()\n",
    "        else:\n",
    "            raise TypeError(f\"Sequenzformat {type(seq)} nicht erkannt!\")\n",
    "\n",
    "        seq = self.pad_channels(seq)\n",
    "\n",
    "        label_full = row[self.label_col]\n",
    "        onset_label = label_full[1]\n",
    "        return seq, torch.tensor(onset_label, dtype=torch.float32)\n",
    "\n",
    "    def pad_channels(self, tensor):\n",
    "        seq_len, channels = tensor.shape\n",
    "        if channels == self.n_channels:\n",
    "            return tensor\n",
    "        elif channels > self.n_channels:\n",
    "            raise RuntimeError(f\"Tensors Kanäle ({channels}) größer als erwartet ({self.n_channels})\")\n",
    "        else:\n",
    "            padding = torch.zeros(seq_len, self.n_channels - channels)\n",
    "            return torch.cat([tensor, padding], dim=1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])\n",
    "    padded_seqs = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_seqs, lengths, labels\n",
    "\n",
    "\n",
    "class EEGOnsetLSTM(nn.Module):\n",
    "    def __init__(self, n_channels, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_channels,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        lengths_sorted, sorted_idx = lengths.sort(descending=True)\n",
    "        x_sorted = x[sorted_idx]\n",
    "\n",
    "        packed_input = pack_padded_sequence(x_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        hn = hn.view(self.lstm.num_layers, 2, x.size(0), self.lstm.hidden_size)\n",
    "        hn_last_layer = hn[-1]\n",
    "        hn_forward = hn_last_layer[0]\n",
    "        hn_backward = hn_last_layer[1]\n",
    "        hn_cat = torch.cat([hn_forward, hn_backward], dim=1)\n",
    "\n",
    "        _, original_idx = sorted_idx.sort()\n",
    "        hn_cat = hn_cat[original_idx]\n",
    "\n",
    "        output = self.fc(hn_cat).squeeze(1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(batch_dir, n_channels, epochs=10, lr=1e-3, batch_size=32, device='cpu'):\n",
    "    dataset = LazyEEGDataset(batch_dir, n_channels, fixed_batch_size=64)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "    model = EEGOnsetLSTM(n_channels).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, lengths, y in dataloader:\n",
    "            x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            epoch_loss += loss.item() * x.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Beispiel-Aufruf\n",
    "batch_dir = \"batches_LSTM\"\n",
    "n_channels = 21\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trained_model = train_model(batch_dir, n_channels, epochs=10, batch_size=8, device=device)\n",
    "torch.save(model.state_dict(), 'eeg_onset_lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851d498-ecad-4069-bcab-0aabaac105e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8356a-fb95-4e82-a56c-fe8537bf67a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
