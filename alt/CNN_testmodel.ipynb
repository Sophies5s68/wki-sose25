{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0145fb-daf1-4136-a7c3-a66c3f8b5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import signal as sig\n",
    "from wettbewerb import load_references\n",
    "import CNN_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be78708-1e82-4c51-851f-03efc7635b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = \"../shared_data/training_mini\"\n",
    "ids, channels, data, sampling_frequencies, reference_systems, eeg_labels = load_references(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430e409-d9c8-49ee-b425-3298fe0750e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_dataset.create_cnn_dataset_map(ids, channels, data, sampling_frequencies, reference_systems, eeg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4120e2-6e99-4a3a-8169-5c8558f7f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "dataset = torch.load(\"cnn_map_dataset.pt\")\n",
    "x, y = dataset[12]\n",
    "\n",
    "print(\"Label:\", y.item() if torch.is_tensor(y) else y)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Feature map:\\n\", x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547265c-f8c9-4c1c-93cc-12c707685d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "dataset_path = \"cnn_map_dataset.pt\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(\"Run CNN_dataset.py, um Dataset zu erstellen\")\n",
    "    \n",
    "dataset = torch.load(dataset_path)\n",
    "print(\"Dataset geladen\")\n",
    "labels = [int(tens[1].item()) for tens in dataset]\n",
    "print(\"Label distribution:\", Counter(labels))\n",
    "\n",
    "# durch Fensterung kommt es zu viel mehr negativen Samples\n",
    "positive = [g for g in dataset if g[1].item() == 1]\n",
    "negative = [g for g in dataset if g[1].item() == 0]\n",
    "\n",
    "# Gleich viele negative wie positive behalten\n",
    "negative = random.sample(negative, len(positive))\n",
    "print(f\"postiv {len(positive)}, negative {len(negative)}\")\n",
    "balanced_data = positive + negative\n",
    "random.shuffle(balanced_data)\n",
    "\n",
    "# Train Test split the Data \n",
    "train_size = int(0.8 * len(balanced_data))\n",
    "test_size = len(balanced_data) - train_size\n",
    "torch.manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(balanced_data, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size =32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size =32, shuffle = False)\n",
    "\n",
    "\n",
    "import CNN_model\n",
    "\n",
    "model = CNN_model.CNN_EEG(in_channels=9, n_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn)\n",
    "    test_acc, _, _ = CNN_model.evaluate_model(model, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "torch.save(model, \"small_trained_cnn_weights.pth\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699990a1-d211-440e-8cfb-ac6c13ca710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score\n",
    "import csv\n",
    "import numpy as np\n",
    "# Datenordner einladen:\n",
    "data_folder = \"data_long\"  \n",
    "file_paths = sorted(glob(os.path.join(data_folder, \"*.pt\")))\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\"Unterordner nicht gefunden\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Modell instantiieren\n",
    "import CNN_model\n",
    "importlib.reload(CNN_model)\n",
    "model = CNN_model.CNN_EEG(in_channels=9, n_classes=2)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "batch_nr = 0\n",
    "train_dataset_global = []\n",
    "test_dataset_global =[]\n",
    "for file_path in file_paths:\n",
    "    \n",
    "    print(f\"Lade Dataset {batch_nr}\")\n",
    "    dataset = torch.load(file_path)\n",
    "    print(f\"Dataset {batch_nr} geladen\")\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    # Train Test split the Data \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    torch.manual_seed(42)\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    test_dataset_global.append(test_dataset)\n",
    "    train_dataset_global.append(train_dataset)\n",
    "    \n",
    "    batch_nr = batch_nr +1\n",
    "    \n",
    "\n",
    "train_dataset_global = ConcatDataset(train_dataset_global)\n",
    "test_dataset_global = ConcatDataset(test_dataset_global)\n",
    "train_loader = DataLoader(train_dataset_global, batch_size =32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset_global, batch_size =32, shuffle = False)\n",
    "\n",
    "# Dynamische Gewichtung der Klassen\n",
    "all_labels = [label.item() for _, label in train_dataset_global]\n",
    "label_counts = Counter(all_labels)\n",
    "total_samples = sum(label_counts.values())\n",
    "num_classes = len(label_counts)\n",
    "\n",
    "weights = [total_samples / (num_classes * label_counts[i]) for i in range(num_classes)]\n",
    "weights[1] *= 3.0\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Klassenverteilung: {label_counts}\")\n",
    "print(f\"Dynamische Verlustgewichte: {weights}\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "    \n",
    "for x, y in train_loader:\n",
    "    print(\"x NaN:\", torch.isnan(x).any())\n",
    "    print(\"x Inf:\", torch.isinf(x).any())\n",
    "    print(\"y NaN:\", torch.isnan(y).any())\n",
    "    print(\"y Inf:\", torch.isinf(y).any())\n",
    "    print(\"x stats - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item(), \"std:\", x.std().item())\n",
    "    break\n",
    "\n",
    "print(f\"starting training on {device}\")   \n",
    "\n",
    "#Training \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn,device)\n",
    "    test_acc, y_true, y_pred = CNN_model.evaluate_model(model, test_loader,device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    " \n",
    "torch.save(model, \"trained_cnn_1.pth\")\n",
    "print(\"finished, model saved\")\n",
    "\n",
    "# Display Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Kein Anfall\", \"Anfall\"])\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "#Trainingsverlauf plotten\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "#Prozess plotten\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "hyperparams = {\n",
    "    \"model\": \"trained_cnn_1\", #Händisch anpassen\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'], \n",
    "    \"weight_decay\": 0.0,\n",
    "    \"loss_function\": \"CrossEntropyLoss\", #muss händisch angepasst werden jenachdem welcher loss verwendet worden ist\n",
    "    \"class_weights\": weights.cpu().numpy().tolist(),  # if using torch weights\n",
    "    \"oversampling\": 3.0, #Händisch anpassen, Wert von Zeile 66\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": 32,\n",
    "    \"threshold\": 0.3,  # if applicable\n",
    "    \"gamma\": None,  # set to 2.0 if using FocalLoss\n",
    "    \"notes\": \"Balanced CE with dynamic weights\",\n",
    "    \"f1_score\": f1_score(y_true, y_pred, average='binary')\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "file_path = \"results/hyperparameters_log.csv\"\n",
    "\n",
    "# Write header only if file does not exist\n",
    "write_header = not os.path.exists(file_path)\n",
    "\n",
    "with open(file_path, mode='a', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=hyperparams.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128d07fe-c22a-4366-8248-4e5e291c63bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Class 0: weight = 0.5108\n",
      "Class 1: weight = 23.5518\n",
      "x NaN: tensor(False)\n",
      "x Inf: tensor(False)\n",
      "y NaN: tensor(False)\n",
      "y Inf: tensor(False)\n",
      "x stats - min: -4.2335662841796875 max: 4.242640495300293 mean: 0.004964651074260473 std: 0.8622854948043823\n",
      "starting training on cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 141\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#Training \u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 141\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     test_acc, y_true, y_pred \u001b[38;5;241m=\u001b[39m CNN_model\u001b[38;5;241m.\u001b[39mevaluate_model(model, test_loader,device)\n\u001b[1;32m    144\u001b[0m     fold_train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/wki-sose25/CNN_model.py:62\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinimum:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum:\u001b[39m\u001b[38;5;124m\"\u001b[39m, out\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUngültige Werte im Modell-Output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/.conda/envs/wki-sose25/lib/python3.8/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stratified group split\n",
    "\n",
    "import random\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset, Subset,TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "# Datenordner einladen:\n",
    "data_folder = \"data_test\"\n",
    "file_paths = sorted(glob(os.path.join(data_folder, \"*.pt\")))\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\"Unterordner nicht gefunden\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Modell instantiieren\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "metrics = []\n",
    "batch_nr = 0\n",
    "train_dataset_global = []\n",
    "test_dataset_global =[]\n",
    "\n",
    "all_x = []\n",
    "all_y = []\n",
    "all_id = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    dataset = torch.load(file_path)\n",
    "    for x, y, gruppe in dataset:\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        all_x.append(x)\n",
    "        all_y.append(int(y))\n",
    "        all_id.append(gruppe)\n",
    "        batch_nr = batch_nr + 1\n",
    "\n",
    "# In NumPy konvertieren\n",
    "all_x_np = np.stack([x.numpy() for x in all_x])\n",
    "all_y_np = np.array(all_y)\n",
    "all_id_np = np.array(all_id)\n",
    "    \n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'x': list(all_x_np),  # wichtig: Liste von Arrays\n",
    "    'y': all_y_np,\n",
    "    'id': all_id_np\n",
    "})\n",
    "# stratified == erhält Klassengewichtung für alle Folds und Groupkfold = keine Überschneidung Patienten\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "num_epochs = 30\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(df['x'], df['y'], df['id'])):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    '''\n",
    "    # Balancieren der Testdaten\n",
    "    train_pos = train_df[train_df['y'] == 1]\n",
    "    train_neg = train_df[train_df['y'] == 0].sample(len(train_pos), random_state=42)\n",
    "    train_bal = pd.concat([train_pos, train_neg]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    X_train = np.stack(train_bal['x'].values)\n",
    "    y_train = train_bal['y'].values\n",
    "    \n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    '''\n",
    "    X_train = np.stack(train_df['x'].values)\n",
    "    y_train = train_df['y'].values\n",
    "    \n",
    "    X_test = np.stack(test_df['x'].values)\n",
    "    y_test = test_df['y'].values\n",
    "    \n",
    "    # Berechnung der Klassengewichte\n",
    "    classes = np.unique(all_y_np)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    for cls, w in zip(classes, weights):\n",
    "        print(f\"Class {cls}: weight = {w:.4f}\")\n",
    "\n",
    "    #Modell instantiieren\n",
    "    import CNN_model\n",
    "    importlib.reload(CNN_model)\n",
    "    model = CNN_model.CNN_EEG(in_channels=18, n_classes=2)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 1e-4)\n",
    "    num_epochs = 30\n",
    "\n",
    "\n",
    "    # Wenn X_train und y_train numpy arrays sind:\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).long()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Gleiches für Testdaten:\n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        print(\"x NaN:\", torch.isnan(x).any())\n",
    "        print(\"x Inf:\", torch.isinf(x).any())\n",
    "        print(\"y NaN:\", torch.isnan(y).any())\n",
    "        print(\"y Inf:\", torch.isinf(y).any())\n",
    "        print(\"x stats - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item(), \"std:\", x.std().item())\n",
    "        break\n",
    "\n",
    "    print(f\"starting training on {device}\")\n",
    "    \n",
    "    # Metrics tracking\n",
    "    fold_train_losses = []\n",
    "    fold_train_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "\n",
    "    #Training \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn,device)\n",
    "        test_acc, y_true, y_pred = CNN_model.evaluate_model(model, test_loader,device)\n",
    "        \n",
    "        fold_train_losses.append(train_loss)\n",
    "        fold_train_accuracies.append(train_acc)\n",
    "        fold_test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Save metrics for this fold\n",
    "    train_losses.append(fold_train_losses)\n",
    "    train_accuracies.append(fold_train_accuracies)\n",
    "    test_accuracies.append(fold_test_accuracies)\n",
    "     \n",
    "    # Confusion Matrix of one fold\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics.append((test_acc,train_acc,y_pred,cm))\n",
    "\n",
    "    print(f\"Metrics last epoch,fold: {fold} test_acc: {test_acc}, train_acc: {train_acc}\")\n",
    "    \n",
    "    \n",
    "    data = data_folder.split(\"/\")[1]\n",
    "    path = \"models_strat/\"\n",
    "    save_path = path + data #Hier ändern für Ordner\n",
    "    os.makedirs(save_path, exist_ok=True)  # Verzeichnis erstellen, falls es noch nicht existiert\n",
    "\n",
    "    torch.save(model, os.path.join(save_path, f\"model_{fold}.pth\"))\n",
    "\n",
    "print(\"finished training\")\n",
    "\n",
    "#Print final metrics and confusion matrix\n",
    "for fold, (test_acc, train_acc, y_pred, cm) in enumerate(metrics):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(f\"  Test accuracy:  {test_acc:.2f}\")\n",
    "    print(f\"  Train accuracy: {train_acc:.2f}\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Kein Anfall\", \"Anfall\"])\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title(\"Confusion Matrix (Test Set)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics per fold\n",
    "epochs = list(range(1, num_epochs + 1))\n",
    "for fold in range(len(train_losses)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses[fold], label='Train Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Loss - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accuracies[fold], label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies[fold], label='Test Accuracy')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Accuracy - Fold {fold+1}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot average across folds\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_train_acc = np.mean(train_accuracies, axis=0)\n",
    "mean_test_acc = np.mean(test_accuracies, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, mean_train_loss, label='Avg Train Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Average Training Loss Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, mean_train_acc, label='Avg Train Accuracy')\n",
    "plt.plot(epochs, mean_test_acc, label='Avg Test Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Average Training Accuracy Across Folds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e88ca556-8902-4893-9764-c539662a7918",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flattened_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCNN_model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(CNN_model)\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCNN_EEG\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m) \u001b[38;5;66;03m# weight decay hinzugefügt, dadurch wesentlich besser\u001b[39;00m\n",
      "File \u001b[0;32m~/wki-sose25/CNN_model.py:22\u001b[0m, in \u001b[0;36mCNN_EEG.__init__\u001b[0;34m(self, in_channels, n_classes)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout2d(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_flattened_size(in_channels), \u001b[38;5;241m256\u001b[39m) \u001b[38;5;66;03m# Eingangsgröße muss angepasst werden\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 22\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mflattened_size\u001b[49m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     23\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     24\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     25\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m256\u001b[39m, n_classes)\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flattened_size' is not defined"
     ]
    }
   ],
   "source": [
    "#ausprobieren 12.06, einlesen und trainieren der fertigen Datensätze\n",
    "\n",
    "import random\n",
    "import importlib\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Modell instantiieren\n",
    "import CNN_model\n",
    "importlib.reload(CNN_model)\n",
    "model = CNN_model.CNN_EEG(in_channels=9, n_classes=2)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4) # weight decay hinzugefügt, dadurch wesentlich besser\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "#einlesen der davor abgespeicherten Datensätze\n",
    "train_dataset_global= torch.load('data_plv/train_dataset.pt')\n",
    "test_dataset_global = torch.load('data_plv/test_dataset.pt')\n",
    "\n",
    "train_loader = DataLoader(train_dataset_global, batch_size =32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset_global, batch_size =32, shuffle = False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "for x, y in train_loader:\n",
    "    print(\"x NaN:\", torch.isnan(x).any())\n",
    "    print(\"x Inf:\", torch.isinf(x).any())\n",
    "    print(\"y NaN:\", torch.isnan(y).any())\n",
    "    print(\"y Inf:\", torch.isinf(y).any())\n",
    "    print(\"x stats - min:\", x.min().item(), \"max:\", x.max().item(), \"mean:\", x.mean().item(), \"std:\", x.std().item())\n",
    "    break\n",
    "\n",
    "print(f\"starting training on {device}\")   \n",
    "\n",
    "#Training \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = CNN_model.train_model(model, train_loader, optimizer, loss_fn,device)\n",
    "    test_acc, y_true, y_pred = CNN_model.evaluate_model(model, test_loader,device)\n",
    "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "train_losses.append(train_loss)\n",
    "train_accuracies.append(train_acc)\n",
    "test_accuracies.append(test_acc)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    " \n",
    "torch.save(model, \"trained_cnn_balanced_12_06_full.pth\")\n",
    "print(\"finished, model saved\")\n",
    "\n",
    "# Display Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Kein Anfall\", \"Anfall\"])\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "#Trainingsverlauf plotten\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "#Prozess plotten\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "hyperparams = {\n",
    "    \"model\": \"trained_cnn_balanced_12_06_full.pth\", #Händisch anpassen\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'], \n",
    "    \"weight_decay\": 0.0,\n",
    "    \"loss_function\": \"CrossEntropyLoss\", #muss händisch angepasst werden jenachdem welcher loss verwendet worden ist\n",
    "    \"class_weights\": \"balanced\",  # if using torch weights\n",
    "    \"oversampling\": 3.0, #Händisch anpassen, Wert von Zeile 66\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": 32,\n",
    "    \"threshold\": 0.3,  # if applicable\n",
    "    \"gamma\": None,  # set to 2.0 if using FocalLoss\n",
    "    \"notes\": \"Balanced CE with dynamic weights\",\n",
    "    \"f1_score\": f1_score(y_true, y_pred, average='binary')\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "file_path = \"results/hyperparameters_log.csv\"\n",
    "\n",
    "# Write header only if file does not exist\n",
    "write_header = not os.path.exists(file_path)\n",
    "\n",
    "with open(file_path, mode='a', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=hyperparams.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8e5dfc-2631-4aff-9535-1ba2975f6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.5527,  0.0000,  0.0143,  0.0000],\n",
      "         [ 0.2273,  2.3705, -0.6264, -0.8522, -0.5450],\n",
      "         [-0.3760, -0.8895, -0.5983, -0.9138, -0.4184],\n",
      "         [-0.4766,  2.1705, -0.2644, -0.5301,  1.9257],\n",
      "         [ 0.0000, -0.4653,  0.0000, -0.3050,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.4063,  0.0000, -0.3615,  0.0000],\n",
      "         [-0.4719,  1.4411, -1.0062, -0.9261, -0.2618],\n",
      "         [ 0.2109, -0.9368, -1.0088, -0.8737,  0.3858],\n",
      "         [ 0.2590,  2.3627, -0.4466, -0.7073,  2.2430],\n",
      "         [ 0.0000,  0.4830,  0.0000,  0.0216,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.2605,  0.0000, -0.0735,  0.0000],\n",
      "         [-0.4514,  1.5096, -0.8715, -0.8254, -0.3707],\n",
      "         [ 0.1451, -0.7477, -0.9319, -0.9477,  0.1590],\n",
      "         [-0.2988,  2.6871, -0.6023, -0.5802,  2.0810],\n",
      "         [ 0.0000,  0.3920,  0.0000, -0.0121,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0723,  0.0000, -0.0048,  0.0000],\n",
      "         [-0.3192,  1.7654, -0.8478, -0.8876, -0.5031],\n",
      "         [ 0.0613, -0.6298, -0.7827, -0.8668,  0.0638],\n",
      "         [-0.4732,  3.0155, -0.1213, -0.6526,  1.5026],\n",
      "         [ 0.0000,  0.1938,  0.0000, -0.4412,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.3408,  0.0000,  0.2491,  0.0000],\n",
      "         [-0.1715,  1.8921, -0.7870, -0.8973, -0.5348],\n",
      "         [-0.5191, -0.5724, -0.8235, -0.7810,  0.5403],\n",
      "         [-0.7554,  2.7169, -0.1579, -0.6010,  1.6368],\n",
      "         [ 0.0000, -0.3293,  0.0000, -0.4458,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.6897,  0.0000,  0.1558,  0.0000],\n",
      "         [ 0.3923,  2.1839, -0.7137, -1.0455, -0.4924],\n",
      "         [-0.2589, -1.0575, -0.6405, -1.1400, -0.3029],\n",
      "         [-0.4297,  2.1133, -0.1781, -0.5610,  1.8834],\n",
      "         [ 0.0000, -0.4037,  0.0000, -0.1944,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.3808,  0.0000, -0.2823,  0.0000],\n",
      "         [-0.4658,  1.4154, -1.1650, -1.0302, -0.1495],\n",
      "         [ 0.3670, -1.0582, -1.1602, -0.9540,  0.5504],\n",
      "         [ 0.4028,  2.1809, -0.3900, -0.7249,  2.0717],\n",
      "         [ 0.0000,  0.6248,  0.0000,  0.1479,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.1505,  0.0000,  0.0598,  0.0000],\n",
      "         [-0.3862,  1.5732, -0.9882, -0.9356, -0.2777],\n",
      "         [ 0.2824, -0.7958, -1.0762, -1.1547,  0.2943],\n",
      "         [-0.2788,  2.4576, -0.5945, -0.6479,  1.9840],\n",
      "         [ 0.0000,  0.5420,  0.0000,  0.0928,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.1016,  0.0000,  0.1521,  0.0000],\n",
      "         [-0.2465,  1.8137, -0.9666, -1.0634, -0.5180],\n",
      "         [ 0.1326, -0.6729, -0.9370, -1.0077,  0.1384],\n",
      "         [-0.4771,  2.7253,  0.0190, -0.6758,  1.5936],\n",
      "         [ 0.0000,  0.2924,  0.0000, -0.4038,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.4504,  0.0000,  0.3459,  0.0000],\n",
      "         [-0.0543,  1.9038, -0.8603, -0.9944, -0.5123],\n",
      "         [-0.5027, -0.6234, -0.9043, -0.8431,  0.5943],\n",
      "         [-0.7641,  2.5272, -0.1098, -0.5813,  1.7044],\n",
      "         [ 0.0000, -0.3229,  0.0000, -0.4530,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -1.6545,  0.0000, -0.6794,  0.0000],\n",
      "         [-1.3191, -1.6116, -0.3026,  0.8584,  0.6963],\n",
      "         [ 0.4943,  1.8756,  0.0949,  1.4919,  1.2577],\n",
      "         [ 0.3824, -0.8812, -0.2398,  0.1288, -1.0967],\n",
      "         [ 0.0000,  0.6023,  0.0000, -0.0977,  0.0000]]])\n",
      "tensor([[[ 0.0000,  0.5527,  0.0000,  0.0143,  0.0000],\n",
      "         [ 0.2273,  2.3705, -0.6264, -0.8522, -0.5450],\n",
      "         [-0.3760, -0.8895, -0.5983, -0.9138, -0.4184],\n",
      "         [-0.4766,  2.1705, -0.2644, -0.5301,  1.9257],\n",
      "         [ 0.0000, -0.4653,  0.0000, -0.3050,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.4063,  0.0000, -0.3615,  0.0000],\n",
      "         [-0.4719,  1.4411, -1.0062, -0.9261, -0.2618],\n",
      "         [ 0.2109, -0.9368, -1.0088, -0.8737,  0.3858],\n",
      "         [ 0.2590,  2.3627, -0.4466, -0.7073,  2.2430],\n",
      "         [ 0.0000,  0.4830,  0.0000,  0.0216,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.2605,  0.0000, -0.0735,  0.0000],\n",
      "         [-0.4514,  1.5096, -0.8715, -0.8254, -0.3707],\n",
      "         [ 0.1451, -0.7477, -0.9319, -0.9477,  0.1590],\n",
      "         [-0.2988,  2.6871, -0.6023, -0.5802,  2.0810],\n",
      "         [ 0.0000,  0.3920,  0.0000, -0.0121,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0723,  0.0000, -0.0048,  0.0000],\n",
      "         [-0.3192,  1.7654, -0.8478, -0.8876, -0.5031],\n",
      "         [ 0.0613, -0.6298, -0.7827, -0.8668,  0.0638],\n",
      "         [-0.4732,  3.0155, -0.1213, -0.6526,  1.5026],\n",
      "         [ 0.0000,  0.1938,  0.0000, -0.4412,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.3408,  0.0000,  0.2491,  0.0000],\n",
      "         [-0.1715,  1.8921, -0.7870, -0.8973, -0.5348],\n",
      "         [-0.5191, -0.5724, -0.8235, -0.7810,  0.5403],\n",
      "         [-0.7554,  2.7169, -0.1579, -0.6010,  1.6368],\n",
      "         [ 0.0000, -0.3293,  0.0000, -0.4458,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.6897,  0.0000,  0.1558,  0.0000],\n",
      "         [ 0.3923,  2.1839, -0.7137, -1.0455, -0.4924],\n",
      "         [-0.2589, -1.0575, -0.6405, -1.1400, -0.3029],\n",
      "         [-0.4297,  2.1133, -0.1781, -0.5610,  1.8834],\n",
      "         [ 0.0000, -0.4037,  0.0000, -0.1944,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.3808,  0.0000, -0.2823,  0.0000],\n",
      "         [-0.4658,  1.4154, -1.1650, -1.0302, -0.1495],\n",
      "         [ 0.3670, -1.0582, -1.1602, -0.9540,  0.5504],\n",
      "         [ 0.4028,  2.1809, -0.3900, -0.7249,  2.0717],\n",
      "         [ 0.0000,  0.6248,  0.0000,  0.1479,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.1505,  0.0000,  0.0598,  0.0000],\n",
      "         [-0.3862,  1.5732, -0.9882, -0.9356, -0.2777],\n",
      "         [ 0.2824, -0.7958, -1.0762, -1.1547,  0.2943],\n",
      "         [-0.2788,  2.4576, -0.5945, -0.6479,  1.9840],\n",
      "         [ 0.0000,  0.5420,  0.0000,  0.0928,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.1016,  0.0000,  0.1521,  0.0000],\n",
      "         [-0.2465,  1.8137, -0.9666, -1.0634, -0.5180],\n",
      "         [ 0.1326, -0.6729, -0.9370, -1.0077,  0.1384],\n",
      "         [-0.4771,  2.7253,  0.0190, -0.6758,  1.5936],\n",
      "         [ 0.0000,  0.2924,  0.0000, -0.4038,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.4504,  0.0000,  0.3459,  0.0000],\n",
      "         [-0.0543,  1.9038, -0.8603, -0.9944, -0.5123],\n",
      "         [-0.5027, -0.6234, -0.9043, -0.8431,  0.5943],\n",
      "         [-0.7641,  2.5272, -0.1098, -0.5813,  1.7044],\n",
      "         [ 0.0000, -0.3229,  0.0000, -0.4530,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -1.6545,  0.0000, -0.6794,  0.0000],\n",
      "         [-1.3191, -1.6116, -0.3026,  0.8584,  0.6963],\n",
      "         [ 0.4943,  1.8756,  0.0949,  1.4919,  1.2577],\n",
      "         [ 0.3824, -0.8812, -0.2398,  0.1288, -1.0967],\n",
      "         [ 0.0000,  0.6023,  0.0000, -0.0977,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Lade die Dateien\n",
    "data1 = torch.load(\"data_30sec/cnn_map_dataset_0.pt\")\n",
    "data2 = torch.load(\"datasets/win30_step30/cnn_map_dataset_0.pt\")\n",
    "\n",
    "# Vergleich: gleiche Länge?\n",
    "print(data1[0][0])\n",
    "print(data2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98b908-7c35-48fe-885e-33927a9c0753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wki-sose25)",
   "language": "python",
   "name": "wki-sose25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
